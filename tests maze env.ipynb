{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze_env import MazeEnv_v0\n",
    "from env_utils.PettingZooEnv_new import PettingZooEnv_new\n",
    "import supersuit\n",
    "import numpy as np\n",
    "from tianshou.env.utils import PettingZooEnv\n",
    "import tianshou as ts\n",
    "from tianshou.utils.net.common import Net\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gymnasium as gym\n",
    "\n",
    "def preprocess_maze_env(render_mode=None, size=6):\n",
    "    env = MazeEnv_v0.env(render_mode=render_mode, size=size)\n",
    "    env = supersuit.multiagent_wrappers.pad_observations_v0(env)\n",
    "    env = PettingZooEnv_new(env)\n",
    "    return env\n",
    "\n",
    "# create a CNN for the observer\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # assume maze size of 6x6 (13x13 with walls)\n",
    "            nn.Conv2d(3, 12, 3), nn.ReLU(inplace=True), nn.MaxPool2d(2,2,1), # (13-3)+1 = 11, (11-2+1)/2+1 = 6\n",
    "            nn.Conv2d(12, 20, 3), nn.ReLU(inplace=True), nn.MaxPool2d(2,2), # 6-3+1=4, (4-2)/2+1 = 2\n",
    "            nn.Flatten(), nn.Linear(20*2*2, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64), nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "        self.batch = obs.shape[0]\n",
    "        #logits = self.model(obs.view(batch, -1))\n",
    "        logits = self.model(obs)\n",
    "        return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_train, eps_test = 0.9, 0.1\n",
    "eps_decay, eps_min = 0.9995, 0.1\n",
    "lr, epoch, batch_size = 1e-4, 60, 64\n",
    "gamma, n_step, target_update_freq = 0.9, 3, 320\n",
    "train_num, test_num = 10, 10\n",
    "buffer_size = 20000\n",
    "step_per_epoch, step_per_collect = 10000, 150\n",
    "maze_width = 6\n",
    "\n",
    "#logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn_no_abs_no_interleaving_cnn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Desktop\\University\\Exeter\\ECMM451 Final Year Project\\Final_Year_Project\\maze_env\\envs\\mazelib.py:32: DeprecationWarning: Seeding based on hashing is deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version. The only \n",
      "supported seed types are: None, int, float, str, bytes, and bytearray.\n",
      "  random.seed(seed)\n"
     ]
    }
   ],
   "source": [
    "# get the vectorized training/testing environments\n",
    "train_envs = ts.env.DummyVectorEnv([preprocess_maze_env for _ in range(train_num)])\n",
    "test_envs = ts.env.DummyVectorEnv([preprocess_maze_env for _ in range(test_num)])\n",
    "\n",
    "env = preprocess_maze_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Desktop\\University\\Exeter\\ECMM451 Final Year Project\\Final_Year_Project\\maze_env\\envs\\mazelib.py:32: DeprecationWarning: Seeding based on hashing is deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version. The only \n",
      "supported seed types are: None, int, float, str, bytes, and bytearray.\n",
      "  random.seed(seed)\n"
     ]
    }
   ],
   "source": [
    "# get agent names\n",
    "agents = env.agents\n",
    "\n",
    "# observation spaces/action spaces for the two agents\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "\n",
    "# define DQN network (128x3 hidden units linear)\n",
    "#net_obs = Net(state_shape, action_shape, [128,128,128])\n",
    "net_obs = Net(state_shape, action_shape, [512, 512, 512])\n",
    "optim_obs = torch.optim.Adam(params=net_obs.parameters(), lr=lr)\n",
    "net_exp = Net(state_shape, action_shape, [16])\n",
    "optim_exp = torch.optim.Adam(params=net_exp.parameters(), lr=lr)\n",
    "\n",
    "# set up policy and collectors\n",
    "agent_observer = ts.policy.DQNPolicy(net_obs, optim_obs, gamma, n_step, target_update_freq)\n",
    "agent_explorer = ts.policy.DQNPolicy(net_exp, optim_exp, gamma, n_step, target_update_freq)\n",
    "agent_policies = [agent_observer, agent_explorer]\n",
    "#agent_policies = [ts.policy.RandomPolicy(), ts.policy.RandomPolicy()] # baseline testing\n",
    "policy = ts.policy.MultiAgentPolicyManager(agent_policies, env)\n",
    "\n",
    "# define the training collector (the calc q and step functions)\n",
    "train_collector = ts.data.Collector(\n",
    "    policy, \n",
    "    train_envs, \n",
    "    ts.data.VectorReplayBuffer(buffer_size, train_num),\n",
    "    exploration_noise=True\n",
    ")\n",
    "\n",
    "# define the testing collector\n",
    "test_collector = ts.data.Collector(\n",
    "    policy, \n",
    "    test_envs,\n",
    "    exploration_noise=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up human render environment\n",
    "env_human = preprocess_maze_env(render_mode=\"human\")\n",
    "env_human = ts.env.DummyVectorEnv([lambda: env_human])\n",
    "human_collector = ts.data.Collector(policy, env_human, exploration_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 6,\n",
       " 'n/st': 100,\n",
       " 'rews': array([[1., 4.],\n",
       "        [1., 4.],\n",
       "        [1., 4.],\n",
       "        [1., 4.],\n",
       "        [1., 4.],\n",
       "        [1., 4.]]),\n",
       " 'lens': array([36, 16, 16, 16, 16, 16]),\n",
       " 'idxs': array([0, 0, 0, 0, 0, 0]),\n",
       " 'rew': 2.5,\n",
       " 'len': 19.333333333333332,\n",
       " 'rew_std': 1.5,\n",
       " 'len_std': 7.453559924999298}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_collector.reset_env(gym_reset_kwargs={\"options\":{\"n_mazes\":1}})\n",
    "human_collector.collect(n_step=100, render=1/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(a[:0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
