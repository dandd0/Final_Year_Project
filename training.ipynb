{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze_env import MazeEnv_v0\n",
    "from utils.PettingZooEnv_new import PettingZooEnv_new\n",
    "import supersuit\n",
    "import numpy as np\n",
    "from tianshou.env.utils import PettingZooEnv\n",
    "import tianshou as ts\n",
    "from tianshou.utils.net.common import Net\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlogger = ts.utils.WandbLogger(train_interval=1, update_interval=1)\\nwriter = SummaryWriter(\\'log/test_maze\\')\\nwriter.add_text(\"run 2\", \"wandb\")\\nlogger.load(writer)\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps_train_start, eps_test = 0.95, 0.0 # exploration rate for training and testing respectively\n",
    "eps_decay, eps_min = 0.999, 0.15 # the exploration rate decay and the minimum exploration rate\n",
    "lr, epochs, batch_size = 5e-4, 150, 512 # the learning rate, max epochs per new maze intro and the update batch size\n",
    "gamma, n_step, target_update_freq = 0.9, 3, 100 # gamma in dqn formula, number of steps to look ahead, number of update calls before updating target network\n",
    "train_num, test_num = 10, 1 # num of simultaneous training and testing environments respectively\n",
    "buffer_size = 30000 # buffer size\n",
    "step_per_epoch, step_per_collect, ep_per_collect = 10000, 200, 1 # number of steps for each epoch, number of steps to collect before updating, number of episodes before updating\n",
    "maze_width = 6 # maze width (not incl. walls)\n",
    "high_eps_run, obs_train, passed_mazes = False, True, True # for random high eps run, for interleaving (might be broken?), whether the policies passed the mazes\n",
    "steps_total, steps_n, episodes_total = 0, 0, 0 # steps count total, steps count within epoch, total number of episodes so far\n",
    "n_mazes, total_mazes = 0, 16 # start with 3 (it will add one later) mazes initially to prevent single maze overfitting, total number of random mazes\n",
    "# for the trivial maze, we use 36 (since it should be 'easier')\n",
    "test_mazes = [] # for printing later\n",
    "threshold_rew = 0.5 # threshold reward to consider a maze passed (tentative value)\n",
    "maze_type = \"random\" # the type of maze to pass into the environment\n",
    "\n",
    "\"\"\"\n",
    "logger = ts.utils.WandbLogger(train_interval=1, update_interval=1)\n",
    "writer = SummaryWriter('log/test_maze')\n",
    "writer.add_text(\"run 2\", \"wandb\")\n",
    "logger.load(writer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\notebook\\utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: dandd0. Use `wandb login --relogin` to force relogin\n",
      "c:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\ipython.py:70: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\Desktop\\University\\Exeter\\ECMM451 Final Year Project\\Final_Year_Project\\wandb\\run-20230626_152213-o3tvr73u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dandd0/Final_Year_Project/runs/o3tvr73u' target=\"_blank\">ruby-capybara-56</a></strong> to <a href='https://wandb.ai/dandd0/Final_Year_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dandd0/Final_Year_Project' target=\"_blank\">https://wandb.ai/dandd0/Final_Year_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dandd0/Final_Year_Project/runs/o3tvr73u' target=\"_blank\">https://wandb.ai/dandd0/Final_Year_Project/runs/o3tvr73u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logger initialization\n",
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    project=\"Final_Year_Project\",\n",
    "    config={\n",
    "        \"eps_train\":eps_train, \"eps_test\":eps_test,\n",
    "        \"eps_decay\": eps_decay, \"eps_min\":eps_min,\n",
    "        \"learning rate\": lr, \"epochs\": epochs, \"batch_size\":batch_size,\n",
    "        \"gamma\": gamma, \"n_step\":n_step, \"target_update_freq\":target_update_freq,\n",
    "        \"buffer_size\":buffer_size,\n",
    "        \"step_per_epoch\":step_per_epoch, \"step_per_collect\":step_per_collect, \"ep_per_collect\":ep_per_collect,\n",
    "        \"maze width\": maze_width, \"n_mazes\":n_mazes, \"total_mazes\":total_mazes,\n",
    "        \"threshold_rew\":threshold_rew, \"maze_type\":maze_type,\n",
    "        \"non-marl\":True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some helper functions\n",
    "def preprocess_maze_env(render_mode=None, size=maze_width):\n",
    "    env = MazeEnv_v0.env_single(render_mode=render_mode, size=size)\n",
    "    env = supersuit.multiagent_wrappers.pad_observations_v0(env)\n",
    "    env = PettingZooEnv_new(env)\n",
    "    return env\n",
    "\n",
    "\"\"\"\n",
    "def preprocess_maze_env(render_mode=None, size=maze_width):\n",
    "    env = MazeEnv_v0.env(render_mode=render_mode, size=size)\n",
    "    env = supersuit.multiagent_wrappers.pad_observations_v0(env)\n",
    "    env = PettingZooEnv_new(env)\n",
    "    return env\n",
    "\"\"\"\n",
    "\n",
    "def interleave_training(obs_train):\n",
    "    if obs_train:\n",
    "        policy.policies[agents[0]].set_eps(eps_train)\n",
    "        policy.policies[agents[1]].set_eps(0)\n",
    "        obs_train = obs_train != True\n",
    "    else:\n",
    "        policy.policies[agents[0]].set_eps(0)\n",
    "        policy.policies[agents[1]].set_eps(eps_train)\n",
    "        obs_train = obs_train != True\n",
    "\n",
    "def set_eps(eps1, eps2=None, single = False):\n",
    "    if single:\n",
    "        policy.set_eps(eps1)\n",
    "    else:\n",
    "        policy.policies[agents[0]].set_eps(eps1)\n",
    "        policy.policies[agents[1]].set_eps(eps2)\n",
    "\n",
    "# create a CNN for the observer\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        lin_size = ((((maze_width*2+1)-3+1)-3+1)-3+1)\n",
    "        self.model = nn.Sequential(\n",
    "            # assume maze size of 6x6 (13x13 with walls)\n",
    "            nn.Conv2d(3, 16, 3), nn.ReLU(inplace=True),  # (13-3)+1 = 11, \n",
    "            nn.Conv2d(16, 32, 3), nn.ReLU(inplace=True), # 11-3+1=9, \n",
    "            nn.Conv2d(32, 64, 3), nn.ReLU(inplace=True), # 9-3+1=7\n",
    "            nn.Flatten(), nn.Linear(64*lin_size*lin_size, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64), nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 5)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "        self.batch = obs.shape[0]\n",
    "        #logits = self.model(obs.view(batch, -1))\n",
    "        logits = self.model(obs)\n",
    "        return logits, state\n",
    "\n",
    "def watch(gym_reset_kwargs):\n",
    "    assert gym_reset_kwargs is not None, \"Please input reset kwargs i.e. options\"\n",
    "    # set policy to eval mode\n",
    "    policy.eval()\n",
    "    human_collector.reset_env(gym_reset_kwargs=gym_reset_kwargs)\n",
    "    #np.random.seed()\n",
    "    human_collector.collect(n_episode=1, render=1/120, gym_reset_kwargs=gym_reset_kwargs)\n",
    "# reset back to training mode\n",
    "    policy.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Desktop\\University\\Exeter\\ECMM451 Final Year Project\\Final_Year_Project\\maze_env\\envs\\mazelib.py:32: DeprecationWarning: Seeding based on hashing is deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version. The only \n",
      "supported seed types are: None, int, float, str, bytes, and bytearray.\n",
      "  random.seed(seed)\n"
     ]
    }
   ],
   "source": [
    "# get the vectorized training/testing environments\n",
    "train_envs = ts.env.DummyVectorEnv([preprocess_maze_env for _ in range(train_num)])\n",
    "test_envs = ts.env.DummyVectorEnv([preprocess_maze_env for _ in range(test_num)])\n",
    "\n",
    "# set up training with no render environment\n",
    "env = preprocess_maze_env()\n",
    "\n",
    "# set up human render environment\n",
    "env_human = preprocess_maze_env(render_mode=\"human\")\n",
    "env_human = ts.env.DummyVectorEnv([lambda: env_human])\n",
    "\n",
    "# get agent names\n",
    "agents = env.agents\n",
    "\n",
    "# observation spaces/action spaces for the two agents\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "\n",
    "# define DQN network (128x3 hidden units linear)\n",
    "#net_obs = Net(state_shape, action_shape, [128,128,128])\n",
    "#net_obs = Net(state_shape, action_shape, [512, 512, 512])\n",
    "net_obs = CNN()\n",
    "optim_obs = torch.optim.Adam(params=net_obs.parameters(), lr=lr)\n",
    "\n",
    "#net_exp = Net(state_shape, action_shape, [8])\n",
    "#optim_exp = torch.optim.Adam(params=net_exp.parameters(), lr=lr)\n",
    "\n",
    "# set up policy and collectors\n",
    "agent_observer = ts.policy.DQNPolicy(net_obs, optim_obs, gamma, n_step, target_update_freq)\n",
    "#agent_explorer = ts.policy.DQNPolicy(net_exp, optim_exp, gamma, n_step, target_update_freq)\n",
    "#agent_policies = [agent_observer, agent_explorer]\n",
    "#agent_policies = [ts.policy.RandomPolicy(), ts.policy.RandomPolicy()] # baseline testing\n",
    "#policy = ts.policy.MultiAgentPolicyManager(agent_policies, env)\n",
    "\n",
    "policy = agent_observer\n",
    "\n",
    "# define the training collector (the calc q and step functions)\n",
    "train_collector = ts.data.Collector(\n",
    "    policy, \n",
    "    train_envs, \n",
    "    ts.data.VectorReplayBuffer(buffer_size, train_num),\n",
    "    exploration_noise=True\n",
    ")\n",
    "\n",
    "# define the testing collector\n",
    "test_collector = ts.data.Collector(\n",
    "    policy, \n",
    "    test_envs,\n",
    "    exploration_noise=True\n",
    ")\n",
    "\n",
    "human_collector = ts.data.Collector(policy, env_human, exploration_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Desktop\\University\\Exeter\\ECMM451 Final Year Project\\Final_Year_Project\\maze_env\\envs\\mazelib.py:32: DeprecationWarning: Seeding based on hashing is deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version. The only \n",
      "supported seed types are: None, int, float, str, bytes, and bytearray.\n",
      "  random.seed(seed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Reward at Epoch 1. Obs: -0.984, Exp: -0.992\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.986, Exp: -0.494\n",
      "Evaluation Reward at Epoch 3. Obs: -0.985, Exp: -0.988\n",
      "Evaluation Reward at Epoch 4. Obs: -0.986, Exp: -0.485\n",
      "Evaluation Reward at Epoch 5. Obs: -0.984, Exp: -0.507\n",
      "Evaluation Reward at Epoch 6. Obs: -0.985, Exp: -0.501\n",
      "Evaluation Reward at Epoch 7. Obs: -0.985, Exp: -0.5\n",
      "Evaluation Reward at Epoch 8. Obs: -0.986, Exp: -0.496\n",
      "Evaluation Reward at Epoch 9. Obs: -0.985, Exp: -0.488\n",
      "Evaluation Reward at Epoch 10. Obs: -0.986, Exp: -0.982\n",
      "Evaluation Reward at Epoch 11. Obs: -0.985, Exp: -0.509\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 12. Obs: -0.986, Exp: -0.481\n",
      "Evaluation Reward at Epoch 13. Obs: -0.985, Exp: -0.481\n",
      "Evaluation Reward at Epoch 14. Obs: -0.984, Exp: -0.505\n",
      "Evaluation Reward at Epoch 15. Obs: -0.997, Exp: -0.498\n",
      "Evaluation Reward at Epoch 16. Obs: -0.985, Exp: -0.496\n",
      "Evaluation Reward at Epoch 17. Obs: -0.984, Exp: -0.023\n",
      "Evaluation Reward at Epoch 18. Obs: -0.993, Exp: -0.501\n",
      "Evaluation Reward at Epoch 19. Obs: -0.989, Exp: -0.488\n",
      "Evaluation Reward at Epoch 20. Obs: -0.989, Exp: -0.492\n",
      "Evaluation Reward at Epoch 21. Obs: -0.986, Exp: -0.015\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 22. Obs: -0.985, Exp: -0.017\n",
      "Evaluation Reward at Epoch 23. Obs: -0.986, Exp: -0.023\n",
      "Evaluation Reward at Epoch 24. Obs: -0.984, Exp: -0.494\n",
      "Evaluation Reward at Epoch 25. Obs: -0.993, Exp: -0.389\n",
      "Evaluation Reward at Epoch 26. Obs: -0.993, Exp: -0.015\n",
      "Evaluation Reward at Epoch 27. Obs: -0.997, Exp: -0.008\n",
      "Evaluation Reward at Epoch 28. Obs: -0.99, Exp: -0.494\n",
      "Evaluation Reward at Epoch 29. Obs: -0.997, Exp: -0.494\n",
      "Evaluation Reward at Epoch 30. Obs: -0.993, Exp: -0.494\n",
      "Evaluation Reward at Epoch 31. Obs: -0.99, Exp: -0.49\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 32. Obs: -0.992, Exp: -0.496\n",
      "Evaluation Reward at Epoch 33. Obs: -0.993, Exp: -0.494\n",
      "Evaluation Reward at Epoch 34. Obs: -0.997, Exp: -0.006\n",
      "Evaluation Reward at Epoch 35. Obs: -0.992, Exp: -0.492\n",
      "Evaluation Reward at Epoch 36. Obs: -0.997, Exp: -0.494\n",
      "Evaluation Reward at Epoch 37. Obs: -0.997, Exp: -0.492\n",
      "Evaluation Reward at Epoch 38. Obs: -0.993, Exp: -0.507\n",
      "Evaluation Reward at Epoch 39. Obs: -0.993, Exp: -0.492\n",
      "Evaluation Reward at Epoch 40. Obs: -0.993, Exp: -0.501\n",
      "Evaluation Reward at Epoch 41. Obs: -0.993, Exp: -0.5\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 42. Obs: -0.993, Exp: -0.496\n",
      "Evaluation Reward at Epoch 43. Obs: -0.993, Exp: -0.49\n",
      "Evaluation Reward at Epoch 44. Obs: -0.997, Exp: -0.494\n",
      "Evaluation Reward at Epoch 45. Obs: -0.986, Exp: -0.49\n",
      "Evaluation Reward at Epoch 46. Obs: -0.997, Exp: -0.496\n",
      "Evaluation Reward at Epoch 47. Obs: -0.997, Exp: -0.995\n",
      "Evaluation Reward at Epoch 48. Obs: -0.993, Exp: -0.5\n",
      "Evaluation Reward at Epoch 49. Obs: -0.99, Exp: -0.494\n",
      "Evaluation Reward at Epoch 50. Obs: -0.993, Exp: -0.992\n",
      "Evaluation Reward at Epoch 51. Obs: -0.985, Exp: -0.486\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 52. Obs: -0.986, Exp: -0.488\n",
      "Evaluation Reward at Epoch 53. Obs: -0.997, Exp: -0.498\n",
      "Evaluation Reward at Epoch 54. Obs: -0.985, Exp: -0.5\n",
      "Evaluation Reward at Epoch 55. Obs: -0.985, Exp: -0.028\n",
      "Evaluation Reward at Epoch 56. Obs: -0.992, Exp: -0.494\n",
      "Evaluation Reward at Epoch 57. Obs: -0.985, Exp: -0.498\n",
      "Evaluation Reward at Epoch 58. Obs: -0.993, Exp: -0.501\n",
      "Evaluation Reward at Epoch 59. Obs: -0.997, Exp: -0.501\n",
      "Evaluation Reward at Epoch 60. Obs: -0.984, Exp: -0.015\n",
      "Evaluation Reward at Epoch 61. Obs: -0.993, Exp: -0.377\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 62. Obs: -0.986, Exp: -0.488\n",
      "Evaluation Reward at Epoch 63. Obs: -0.99, Exp: -0.49\n",
      "Evaluation Reward at Epoch 64. Obs: -0.99, Exp: -0.494\n",
      "Evaluation Reward at Epoch 65. Obs: -0.993, Exp: -0.496\n",
      "Evaluation Reward at Epoch 66. Obs: -0.99, Exp: -0.982\n",
      "Evaluation Reward at Epoch 67. Obs: -0.992, Exp: -0.154\n",
      "Evaluation Reward at Epoch 68. Obs: -0.993, Exp: -0.486\n",
      "Evaluation Reward at Epoch 69. Obs: -0.997, Exp: -0.496\n",
      "Evaluation Reward at Epoch 70. Obs: -0.993, Exp: -0.607\n",
      "Evaluation Reward at Epoch 71. Obs: -0.997, Exp: -0.496\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 72. Obs: -0.993, Exp: -0.5\n",
      "Evaluation Reward at Epoch 73. Obs: -0.99, Exp: -0.984\n",
      "Evaluation Reward at Epoch 74. Obs: -0.993, Exp: -0.492\n",
      "Evaluation Reward at Epoch 75. Obs: -0.986, Exp: -0.481\n",
      "Evaluation Reward at Epoch 76. Obs: -0.99, Exp: -0.494\n",
      "Evaluation Reward at Epoch 77. Obs: -0.993, Exp: -0.443\n",
      "Evaluation Reward at Epoch 78. Obs: -0.988, Exp: -0.8\n",
      "Evaluation Reward at Epoch 79. Obs: -0.989, Exp: -0.494\n",
      "Evaluation Reward at Epoch 80. Obs: -0.993, Exp: -0.336\n",
      "Evaluation Reward at Epoch 81. Obs: -0.993, Exp: -0.37\n",
      "Test Mazes results: [1, 0, 0]\n",
      "Evaluation Reward at Epoch 82. Obs: -0.985, Exp: -0.492\n",
      "Evaluation Reward at Epoch 83. Obs: -0.993, Exp: -0.492\n",
      "Evaluation Reward at Epoch 84. Obs: -0.986, Exp: -0.49\n",
      "Evaluation Reward at Epoch 85. Obs: -0.997, Exp: -0.492\n",
      "Evaluation Reward at Epoch 86. Obs: -0.986, Exp: -0.259\n",
      "Evaluation Reward at Epoch 87. Obs: -0.989, Exp: -0.516\n",
      "Evaluation Reward at Epoch 88. Obs: -0.993, Exp: -0.494\n",
      "Evaluation Reward at Epoch 89. Obs: -0.997, Exp: -0.496\n",
      "Evaluation Reward at Epoch 90. Obs: -0.997, Exp: -0.492\n",
      "Evaluation Reward at Epoch 91. Obs: -0.997, Exp: -0.494\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 92. Obs: -0.992, Exp: -0.372\n",
      "Evaluation Reward at Epoch 93. Obs: -0.992, Exp: -0.984\n",
      "Evaluation Reward at Epoch 94. Obs: -0.993, Exp: -0.494\n",
      "Evaluation Reward at Epoch 95. Obs: -0.985, Exp: -0.479\n",
      "Evaluation Reward at Epoch 96. Obs: -0.997, Exp: -0.992\n",
      "Evaluation Reward at Epoch 97. Obs: -0.993, Exp: -0.99\n",
      "Evaluation Reward at Epoch 98. Obs: -0.992, Exp: -0.901\n",
      "Evaluation Reward at Epoch 99. Obs: -0.997, Exp: -0.5\n",
      "Evaluation Reward at Epoch 100. Obs: -0.993, Exp: -0.486\n",
      "Evaluation Reward at Epoch 101. Obs: -0.993, Exp: -0.997\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 102. Obs: -0.992, Exp: -0.992\n",
      "Evaluation Reward at Epoch 103. Obs: -0.986, Exp: -0.755\n",
      "Evaluation Reward at Epoch 104. Obs: -0.989, Exp: -0.49\n",
      "Evaluation Reward at Epoch 105. Obs: -0.997, Exp: -0.496\n",
      "Evaluation Reward at Epoch 106. Obs: -0.997, Exp: -0.995\n",
      "Evaluation Reward at Epoch 107. Obs: -0.99, Exp: -0.993\n",
      "Evaluation Reward at Epoch 108. Obs: -0.992, Exp: -0.494\n",
      "Evaluation Reward at Epoch 109. Obs: -0.997, Exp: -0.997\n",
      "Evaluation Reward at Epoch 110. Obs: -0.993, Exp: -0.997\n",
      "Evaluation Reward at Epoch 111. Obs: -0.993, Exp: -0.999\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 112. Obs: -0.992, Exp: -0.483\n",
      "Evaluation Reward at Epoch 113. Obs: -0.989, Exp: -0.99\n",
      "Evaluation Reward at Epoch 114. Obs: -0.989, Exp: -0.982\n",
      "Evaluation Reward at Epoch 115. Obs: -0.993, Exp: -0.496\n",
      "Evaluation Reward at Epoch 116. Obs: -0.993, Exp: -0.992\n",
      "Evaluation Reward at Epoch 117. Obs: -0.997, Exp: -0.993\n",
      "Evaluation Reward at Epoch 118. Obs: -0.993, Exp: -0.999\n",
      "Evaluation Reward at Epoch 119. Obs: -0.997, Exp: -0.509\n",
      "Evaluation Reward at Epoch 120. Obs: -0.992, Exp: -0.511\n",
      "Evaluation Reward at Epoch 121. Obs: -0.997, Exp: -0.498\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 122. Obs: -0.993, Exp: -0.915\n",
      "Evaluation Reward at Epoch 123. Obs: -0.988, Exp: -0.973\n",
      "Evaluation Reward at Epoch 124. Obs: -0.989, Exp: -0.99\n",
      "Evaluation Reward at Epoch 125. Obs: -0.99, Exp: -0.993\n",
      "Evaluation Reward at Epoch 126. Obs: -0.993, Exp: -0.999\n",
      "Evaluation Reward at Epoch 127. Obs: -0.986, Exp: -0.98\n",
      "Evaluation Reward at Epoch 128. Obs: -0.993, Exp: -0.496\n",
      "Evaluation Reward at Epoch 129. Obs: -0.989, Exp: -0.99\n",
      "Evaluation Reward at Epoch 130. Obs: -0.99, Exp: -0.986\n",
      "Evaluation Reward at Epoch 131. Obs: -0.989, Exp: -0.821\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 132. Obs: -0.99, Exp: -0.992\n",
      "Evaluation Reward at Epoch 133. Obs: -0.986, Exp: -0.772\n",
      "Evaluation Reward at Epoch 134. Obs: -0.997, Exp: -0.995\n",
      "Evaluation Reward at Epoch 135. Obs: -0.997, Exp: -0.505\n",
      "Evaluation Reward at Epoch 136. Obs: -0.997, Exp: -0.992\n",
      "Evaluation Reward at Epoch 137. Obs: -0.997, Exp: -0.99\n",
      "Evaluation Reward at Epoch 138. Obs: -0.993, Exp: -0.992\n",
      "Evaluation Reward at Epoch 139. Obs: -0.993, Exp: -0.995\n",
      "Evaluation Reward at Epoch 140. Obs: -0.985, Exp: -0.967\n",
      "Evaluation Reward at Epoch 141. Obs: -0.997, Exp: -0.986\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 142. Obs: -0.99, Exp: -0.984\n",
      "Evaluation Reward at Epoch 143. Obs: -0.989, Exp: -0.986\n",
      "Evaluation Reward at Epoch 144. Obs: -0.99, Exp: -0.995\n",
      "Evaluation Reward at Epoch 145. Obs: -0.993, Exp: -0.997\n",
      "Evaluation Reward at Epoch 146. Obs: -0.992, Exp: -0.841\n",
      "Evaluation Reward at Epoch 147. Obs: -0.997, Exp: -0.494\n",
      "Evaluation Reward at Epoch 148. Obs: -0.997, Exp: -0.993\n",
      "Evaluation Reward at Epoch 149. Obs: -0.997, Exp: -0.997\n",
      "Evaluation Reward at Epoch 150. Obs: -0.993, Exp: -0.548\n",
      "Evaluation Reward at Epoch 151. Obs: -0.989, Exp: -0.764\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 152. Obs: -0.993, Exp: -0.988\n",
      "Evaluation Reward at Epoch 153. Obs: -0.993, Exp: -0.511\n",
      "Evaluation Reward at Epoch 154. Obs: -0.997, Exp: -1.001\n",
      "Evaluation Reward at Epoch 155. Obs: -0.997, Exp: -0.995\n",
      "Evaluation Reward at Epoch 156. Obs: -0.986, Exp: -0.995\n",
      "Evaluation Reward at Epoch 157. Obs: -0.997, Exp: -0.993\n",
      "Evaluation Reward at Epoch 158. Obs: -0.995, Exp: -0.993\n",
      "Evaluation Reward at Epoch 159. Obs: -0.993, Exp: -0.65\n",
      "Evaluation Reward at Epoch 160. Obs: -0.993, Exp: -0.648\n",
      "Evaluation Reward at Epoch 161. Obs: -0.993, Exp: -0.522\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 162. Obs: -0.99, Exp: -0.657\n",
      "Evaluation Reward at Epoch 163. Obs: -0.986, Exp: -0.496\n",
      "Evaluation Reward at Epoch 164. Obs: -0.993, Exp: -0.993\n",
      "Evaluation Reward at Epoch 165. Obs: -0.997, Exp: -0.995\n",
      "Evaluation Reward at Epoch 166. Obs: -0.99, Exp: -0.992\n",
      "Evaluation Reward at Epoch 167. Obs: -0.989, Exp: -0.492\n",
      "Evaluation Reward at Epoch 168. Obs: -0.99, Exp: -0.614\n",
      "Evaluation Reward at Epoch 169. Obs: -0.997, Exp: -0.997\n",
      "Evaluation Reward at Epoch 170. Obs: -0.993, Exp: -0.599\n",
      "Evaluation Reward at Epoch 171. Obs: -0.997, Exp: -0.993\n",
      "Test Mazes results: [0, 0, 0]\n",
      "Evaluation Reward at Epoch 172. Obs: -0.997, Exp: -0.997\n",
      "Evaluation Reward at Epoch 173. Obs: -0.99, Exp: -0.986\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Desktop\\University\\Exeter\\ECMM451 Final Year Project\\Final_Year_Project\\training.ipynb Cell 9\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# train the model in training environment\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m train_collector\u001b[39m.\u001b[39mreset_env(gym_reset_kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m\"\u001b[39m:{\u001b[39m\"\u001b[39m\u001b[39mn_mazes\u001b[39m\u001b[39m\"\u001b[39m:n_mazes}})\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m result \u001b[39m=\u001b[39m train_collector\u001b[39m.\u001b[39;49mcollect(n_episode\u001b[39m=\u001b[39;49mep_per_collect, gym_reset_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39moptions\u001b[39;49m\u001b[39m\"\u001b[39;49m:{\u001b[39m\"\u001b[39;49m\u001b[39mn_mazes\u001b[39;49m\u001b[39m\"\u001b[39;49m:n_mazes}})\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m steps_n \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(result[\u001b[39m'\u001b[39m\u001b[39mn/st\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m steps_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(result[\u001b[39m'\u001b[39m\u001b[39mn/st\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\tianshou\\data\\collector.py:278\u001b[0m, in \u001b[0;36mCollector.collect\u001b[1;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m no_grad:\n\u001b[0;32m    276\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# faster than retain_grad version\u001b[39;00m\n\u001b[0;32m    277\u001b[0m         \u001b[39m# self.data.obs will be used by agent to get result\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, last_state)\n\u001b[0;32m    279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    280\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, last_state)\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\tianshou\\policy\\multiagent\\mapolicy.py:145\u001b[0m, in \u001b[0;36mMultiAgentPolicyManager.forward\u001b[1;34m(self, batch, state, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tmp_batch\u001b[39m.\u001b[39mobs_next, \u001b[39m'\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    144\u001b[0m         tmp_batch\u001b[39m.\u001b[39mobs_next \u001b[39m=\u001b[39m tmp_batch\u001b[39m.\u001b[39mobs_next\u001b[39m.\u001b[39mobs\n\u001b[1;32m--> 145\u001b[0m out \u001b[39m=\u001b[39m policy(\n\u001b[0;32m    146\u001b[0m     batch\u001b[39m=\u001b[39mtmp_batch,\n\u001b[0;32m    147\u001b[0m     state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m state[agent_id],\n\u001b[0;32m    148\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    149\u001b[0m )\n\u001b[0;32m    150\u001b[0m act \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mact\n\u001b[0;32m    151\u001b[0m each_state \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mstate \\\n\u001b[0;32m    152\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39mhasattr\u001b[39m(out, \u001b[39m\"\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m out\u001b[39m.\u001b[39mstate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \\\n\u001b[0;32m    153\u001b[0m     \u001b[39melse\u001b[39;00m Batch()\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\tianshou\\policy\\modelfree\\dqn.py:160\u001b[0m, in \u001b[0;36mDQNPolicy.forward\u001b[1;34m(self, batch, state, model, input, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m obs \u001b[39m=\u001b[39m batch[\u001b[39minput\u001b[39m]\n\u001b[0;32m    159\u001b[0m obs_next \u001b[39m=\u001b[39m obs\u001b[39m.\u001b[39mobs \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(obs, \u001b[39m\"\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m obs\n\u001b[1;32m--> 160\u001b[0m logits, hidden \u001b[39m=\u001b[39m model(obs_next, state\u001b[39m=\u001b[39;49mstate, info\u001b[39m=\u001b[39;49mbatch\u001b[39m.\u001b[39;49minfo)\n\u001b[0;32m    161\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_q_value(logits, \u001b[39mgetattr\u001b[39m(obs, \u001b[39m\"\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmax_action_num\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mf:\\Desktop\\University\\Exeter\\ECMM451 Final Year Project\\Final_Year_Project\\training.ipynb Cell 9\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch \u001b[39m=\u001b[39m obs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#logits = self.model(obs.view(batch, -1))\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(obs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits, state\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# manual training loop\n",
    "np.random.seed()\n",
    "\n",
    "\"\"\"\n",
    "# collect a bunch of random ones\n",
    "policy.policies[agents[0]].set_eps(1)\n",
    "policy.policies[agents[1]].set_eps(1)\n",
    "train_collector.collect(n_step=10000)\n",
    "\"\"\"\n",
    "for mazes in range(total_mazes):\n",
    "    if passed_mazes == True:\n",
    "        n_mazes += 1\n",
    "        # reset epsilon again for the new maze\n",
    "        eps_train = 0.9\n",
    "    else:\n",
    "        print(\"Failed to find a solution within suitable time. Stopping training.\")\n",
    "        break\n",
    "\n",
    "    # set first eps\n",
    "    policy.policies[agents[0]].set_eps(eps_train)\n",
    "    policy.policies[agents[1]].set_eps(eps_train)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # reset number of steps couhnt\n",
    "        steps_n = 0 \n",
    "        \n",
    "        # training loop\n",
    "        while steps_n < step_per_epoch:\n",
    "            # have runs where the exploration rate is very high\n",
    "            if np.random.randint(0, 10) == 0:\n",
    "                eps_prev = eps_train\n",
    "                eps_train = 0.9\n",
    "                high_eps_run = True\n",
    "            \n",
    "            policy.policies[agents[0]].set_eps(eps_train)\n",
    "            policy.policies[agents[1]].set_eps(eps_train)\n",
    "\n",
    "            # train the model in training environment\n",
    "            train_collector.reset_env(gym_reset_kwargs={\"options\":{\"n_mazes\":n_mazes}})\n",
    "            result = train_collector.collect(n_episode=ep_per_collect, gym_reset_kwargs={\"options\":{\"n_mazes\":n_mazes}})\n",
    "            steps_n += int(result['n/st'])\n",
    "            steps_total += int(result['n/st'])\n",
    "            \n",
    "            # update the parameters after train_num steps\n",
    "            policy.update(batch_size, train_collector.buffer)\n",
    "\n",
    "            # set the random training epsilon after each steps per collect\n",
    "            # decay it by specified parameter every\n",
    "            eps_train *= eps_decay\n",
    "            eps_train = np.max([eps_train, eps_min])\n",
    "\n",
    "            \"\"\"\n",
    "            # swap training modes of the two agents after 1000 steps (interleaving)\n",
    "            if (int(result['n/st']) + steps_total) % 1000 < steps_total % 1000:\n",
    "                #interleave_training(obs_train)\n",
    "                set_eps(eps_train)\n",
    "            \"\"\"\n",
    "\n",
    "            #  reset high exploration\n",
    "            if high_eps_run:\n",
    "                eps_train = eps_prev\n",
    "                high_eps_run = False\n",
    "\n",
    "            # log\n",
    "            if result[\"n/ep\"] > 0:\n",
    "                log_data = {\"train\":{\n",
    "                        \"episode\": result[\"n/ep\"],\n",
    "                        \"obs_reward\": np.mean(result[\"rews\"][:,0]),\n",
    "                        \"exp_reward\": np.mean(result[\"rews\"][:,1]),\n",
    "                        \"length\": result[\"len\"],\n",
    "                        \"exploration rate\": eps_train\n",
    "                    }\n",
    "                }\n",
    "                wandb.log(data=log_data, step=steps_total)\n",
    "        \n",
    "        #print(f\"Current training epsilon: {np.round(policy.policies[agents[0]].eps, 4)}\")\n",
    "        \n",
    "        # check test results\n",
    "        agent0_eps = policy.policies[agents[0]]\n",
    "        agent1_aps = policy.policies[agents[1]]\n",
    "        policy.policies[agents[0]].set_eps(eps_test)\n",
    "        policy.policies[agents[1]].set_eps(eps_test)\n",
    "        \n",
    "        passed_mazes = True\n",
    "        test_mazes = []\n",
    "        for seed in range(n_mazes):\n",
    "            # test through all previous mazes\n",
    "            test_collector.reset_env(gym_reset_kwargs={\"seed\":seed+1})\n",
    "            test_result = test_collector.collect(n_episode=test_num, gym_reset_kwargs={\"seed\":seed+1})\n",
    "            \n",
    "            # if any of the previous mazes failed, continue\n",
    "            if np.mean(test_result['rews'][:,0]) < threshold_rew:\n",
    "                passed_mazes = False\n",
    "                test_mazes.append(0)\n",
    "            else:\n",
    "                test_mazes.append(1)\n",
    "\n",
    "        # early stop when policy reaches good enough performance\n",
    "        #if np.mean(test_result['rews'][:,0]) >= reward_threshold:\n",
    "        #    break\n",
    "\n",
    "        # log\n",
    "        log_data = {\"test\":{\n",
    "                \"obs_reward\": np.mean(test_result[\"rews\"][:,0]),\n",
    "                \"exp_reward\": np.mean(test_result[\"rews\"][:,1]),\n",
    "                \"length\": test_result[\"len\"],\n",
    "                \"obs_reward_std\": np.std(test_result[\"rews\"][:,0]),\n",
    "                \"exp_reward_std\": np.std(test_result[\"rews\"][:,1])\n",
    "            }\n",
    "        }\n",
    "        wandb.log(data=log_data, step=steps_total)\n",
    "        \n",
    "        print(f\"Evaluation Reward at Epoch {epoch+1}. Obs: {np.round(np.mean(test_result['rews'][:,0]), 3)}, Exp: {np.round(np.mean(test_result['rews'][:,1]), 3)}\")\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Test Mazes results: {test_mazes}\")\n",
    "\n",
    "        # check if the agent can auccessfully solve the maze (within some threshold)\n",
    "        if passed_mazes:\n",
    "            print(f\"Agents solved the current maze and all previous mazes. Current number of mazes: {n_mazes}.\")\n",
    "            print(f\"Solved all mazes on epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "        # every n epochs render the policy for human-based evalution\n",
    "        if (epoch % 50) == 0:\n",
    "\n",
    "            # set policy to eval mode\n",
    "            policy.eval()\n",
    "            for maze in range(n_mazes):\n",
    "                human_collector.reset_env(gym_reset_kwargs={\"seed\":maze+1})\n",
    "                #np.random.seed()\n",
    "                human_collector.collect(n_episode=1, render=1/60)\n",
    "\n",
    "            # reset back to training mode\n",
    "            policy.train()\n",
    "        \n",
    "        # reset eps\n",
    "        policy.policies[agents[0]].set_eps(eps_train)\n",
    "        policy.policies[agents[1]].set_eps(eps_train)\n",
    "        \n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\ipython.py:70: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">genial-pine-32</strong> at: <a href='https://wandb.ai/dandd0/Final_Year_Project/runs/9c5g2okd' target=\"_blank\">https://wandb.ai/dandd0/Final_Year_Project/runs/9c5g2okd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230620_155155-9c5g2okd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(policy.policies[agents[0]].state_dict(), \"model/dqn_baseline_obs.pt\")\n",
    "#torch.save(policy.policies[agents[1]].state_dict(), \"model/dqn_baseline_exp.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Desktop\\University\\Exeter\\ECMM451 Final Year Project\\Final_Year_Project\\maze_env\\envs\\mazelib.py:32: DeprecationWarning: Seeding based on hashing is deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version. The only \n",
      "supported seed types are: None, int, float, str, bytes, and bytearray.\n",
      "  random.seed(seed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Reward at Epoch 1. Obs: -0.972, Exp: -0.006\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.972, Exp: -0.513\n",
      "Evaluation Reward at Epoch 3. Obs: -0.972, Exp: -0.49\n",
      "Evaluation Reward at Epoch 4. Obs: -0.972, Exp: -0.524\n",
      "Evaluation Reward at Epoch 5. Obs: -0.972, Exp: -0.039\n",
      "Evaluation Reward at Epoch 6. Obs: -0.972, Exp: -0.045\n",
      "Evaluation Reward at Epoch 7. Obs: -0.972, Exp: -0.53\n",
      "Evaluation Reward at Epoch 8. Obs: -0.972, Exp: -0.045\n",
      "Evaluation Reward at Epoch 9. Obs: -0.972, Exp: -0.518\n",
      "Evaluation Reward at Epoch 10. Obs: -0.972, Exp: -0.045\n",
      "Evaluation Reward at Epoch 11. Obs: -0.972, Exp: -0.507\n",
      "Test Mazes results: [0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Desktop\\University\\Exeter\\ECMM451 Final Year Project\\Final_Year_Project\\training.ipynb Cell 13\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X15sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# train the model in training environment\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X15sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m train_collector\u001b[39m.\u001b[39mreset_env(gym_reset_kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m:n_mazes})\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X15sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m result \u001b[39m=\u001b[39m train_collector\u001b[39m.\u001b[39;49mcollect(n_episode\u001b[39m=\u001b[39;49mep_per_collect, gym_reset_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m:n_mazes})\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X15sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m steps_n \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(result[\u001b[39m'\u001b[39m\u001b[39mn/st\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Desktop/University/Exeter/ECMM451%20Final%20Year%20Project/Final_Year_Project/training.ipynb#X15sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m steps_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(result[\u001b[39m'\u001b[39m\u001b[39mn/st\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\tianshou\\data\\collector.py:328\u001b[0m, in \u001b[0;36mCollector.collect\u001b[1;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m         time\u001b[39m.\u001b[39msleep(render)\n\u001b[0;32m    327\u001b[0m \u001b[39m# add data into the buffer\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m ptr, ep_rew, ep_len, ep_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer\u001b[39m.\u001b[39;49madd(\n\u001b[0;32m    329\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, buffer_ids\u001b[39m=\u001b[39;49mready_env_ids\n\u001b[0;32m    330\u001b[0m )\n\u001b[0;32m    332\u001b[0m \u001b[39m# collect statistics\u001b[39;00m\n\u001b[0;32m    333\u001b[0m step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(ready_env_ids)\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\tianshou\\data\\buffer\\manager.py:148\u001b[0m, in \u001b[0;36mReplayBufferManager.add\u001b[1;34m(self, batch, buffer_ids)\u001b[0m\n\u001b[0;32m    146\u001b[0m ptrs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(ptrs)\n\u001b[0;32m    147\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_meta[ptrs] \u001b[39m=\u001b[39m batch\n\u001b[0;32m    149\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m     batch\u001b[39m.\u001b[39mrew \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mrew\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\tianshou\\data\\batch.py:267\u001b[0m, in \u001b[0;36mBatch.__setitem__\u001b[1;34m(self, index, value)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    266\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 267\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__dict__\u001b[39;49m[key][index] \u001b[39m=\u001b[39m value[key]\n\u001b[0;32m    268\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(val, Batch):\n",
      "File \u001b[1;32mc:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\tianshou\\data\\batch.py:267\u001b[0m, in \u001b[0;36mBatch.__setitem__\u001b[1;34m(self, index, value)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    266\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 267\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__dict__\u001b[39;49m[key][index] \u001b[39m=\u001b[39m value[key]\n\u001b[0;32m    268\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(val, Batch):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# manual training loop\n",
    "np.random.seed()\n",
    "\n",
    "\"\"\"\n",
    "# collect a bunch of random ones\n",
    "policy.policies[agents[0]].set_eps(1)\n",
    "policy.policies[agents[1]].set_eps(1)\n",
    "train_collector.collect(n_step=10000)\n",
    "\"\"\"\n",
    "for mazes in range(total_mazes):\n",
    "    if passed_mazes == True:\n",
    "        n_mazes += 1\n",
    "        # reset epsilon again for the new maze\n",
    "        eps_train = 0.9\n",
    "    else:\n",
    "        print(\"Failed to find a solution within suitable time. Stopping training.\")\n",
    "        break\n",
    "\n",
    "    # set first eps\n",
    "    policy.policies[agents[0]].set_eps(eps_train)\n",
    "    policy.policies[agents[1]].set_eps(eps_train)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # reset number of steps couhnt\n",
    "        steps_n = 0 \n",
    "        \n",
    "        # training loop\n",
    "        while steps_n < step_per_epoch:\n",
    "            # have runs where the exploration rate is very high\n",
    "            if np.random.randint(0, 10) == 0:\n",
    "                eps_prev = eps_train\n",
    "                eps_train = 0.9\n",
    "                high_eps_run = True\n",
    "            \n",
    "            policy.policies[agents[0]].set_eps(eps_train)\n",
    "            policy.policies[agents[1]].set_eps(eps_train)\n",
    "\n",
    "            # train the model in training environment\n",
    "            train_collector.reset_env(gym_reset_kwargs={\"seed\":n_mazes})\n",
    "            result = train_collector.collect(n_episode=ep_per_collect, gym_reset_kwargs={\"seed\":n_mazes})\n",
    "            steps_n += int(result['n/st'])\n",
    "            steps_total += int(result['n/st'])\n",
    "            \n",
    "            # update the parameters after train_num steps\n",
    "            policy.update(batch_size, train_collector.buffer)\n",
    "\n",
    "            # set the random training epsilon after each steps per collect\n",
    "            # decay it by specified parameter every\n",
    "            eps_train *= eps_decay\n",
    "            eps_train = np.max([eps_train, eps_min])\n",
    "\n",
    "            \"\"\"\n",
    "            # swap training modes of the two agents after 1000 steps (interleaving)\n",
    "            if (int(result['n/st']) + steps_total) % 1000 < steps_total % 1000:\n",
    "                #interleave_training(obs_train)\n",
    "                set_eps(eps_train)\n",
    "            \"\"\"\n",
    "\n",
    "            #  reset high exploration\n",
    "            if high_eps_run:\n",
    "                eps_train = eps_prev\n",
    "                high_eps_run = False\n",
    "\n",
    "            # log\n",
    "            if result[\"n/ep\"] > 0:\n",
    "                log_data = {\"train\":{\n",
    "                        \"episode\": result[\"n/ep\"],\n",
    "                        \"obs_reward\": np.mean(result[\"rews\"][:,0]),\n",
    "                        \"exp_reward\": np.mean(result[\"rews\"][:,1]),\n",
    "                        \"length\": result[\"len\"],\n",
    "                        \"exploration rate\": eps_train\n",
    "                    }\n",
    "                }\n",
    "                wandb.log(data=log_data, step=steps_total)\n",
    "        \n",
    "        #print(f\"Current training epsilon: {np.round(policy.policies[agents[0]].eps, 4)}\")\n",
    "        \n",
    "        # check test results\n",
    "        agent0_eps = policy.policies[agents[0]]\n",
    "        agent1_aps = policy.policies[agents[1]]\n",
    "        policy.policies[agents[0]].set_eps(eps_test)\n",
    "        policy.policies[agents[1]].set_eps(eps_test)\n",
    "        \n",
    "        passed_mazes = True\n",
    "        test_mazes = []\n",
    "        #for seed in range(n_mazes):\n",
    "        # test through just new maze\n",
    "        test_collector.reset_env(gym_reset_kwargs={\"seed\":n_mazes})\n",
    "        test_result = test_collector.collect(n_episode=test_num, gym_reset_kwargs={\"seed\":n_mazes})\n",
    "        \n",
    "        # if any of the previous mazes failed, continue\n",
    "        if np.mean(test_result['rews'][:,0]) < threshold_rew:\n",
    "            passed_mazes = False\n",
    "            test_mazes.append(0)\n",
    "        else:\n",
    "            test_mazes.append(1)\n",
    "\n",
    "        # early stop when policy reaches good enough performance\n",
    "        #if np.mean(test_result['rews'][:,0]) >= reward_threshold:\n",
    "        #    break\n",
    "\n",
    "        # log\n",
    "        log_data = {\"test\":{\n",
    "                \"obs_reward\": np.mean(test_result[\"rews\"][:,0]),\n",
    "                \"exp_reward\": np.mean(test_result[\"rews\"][:,1]),\n",
    "                \"length\": test_result[\"len\"],\n",
    "                \"obs_reward_std\": np.std(test_result[\"rews\"][:,0]),\n",
    "                \"exp_reward_std\": np.std(test_result[\"rews\"][:,1])\n",
    "            }\n",
    "        }\n",
    "        wandb.log(data=log_data, step=steps_total)\n",
    "        \n",
    "        print(f\"Evaluation Reward at Epoch {epoch+1}. Obs: {np.round(np.mean(test_result['rews'][:,0]), 3)}, Exp: {np.round(np.mean(test_result['rews'][:,1]), 3)}\")\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Test Mazes results: {test_mazes}\")\n",
    "\n",
    "        # check if the agent can auccessfully solve the maze (within some threshold)\n",
    "        if passed_mazes:\n",
    "            print(f\"Agents solved the current maze and all previous mazes. Current number of mazes: {n_mazes}.\")\n",
    "            print(f\"Solved all mazes on epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "        # every n epochs render the policy for human-based evalution\n",
    "        if (epoch % 10) == 0:\n",
    "\n",
    "            # set policy to eval mode\n",
    "            policy.eval()\n",
    "            #for maze in range(n_mazes):\n",
    "            human_collector.reset_env(gym_reset_kwargs={\"seed\":n_mazes})\n",
    "            #np.random.seed()\n",
    "            human_collector.collect(n_episode=1, render=1/60)\n",
    "\n",
    "            # reset back to training mode\n",
    "            policy.train()\n",
    "        \n",
    "        # reset eps\n",
    "        policy.policies[agents[0]].set_eps(eps_train)\n",
    "        policy.policies[agents[1]].set_eps(eps_train)\n",
    "        \n",
    "print('Finished Training.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 1,\n",
       " 'n/st': 356,\n",
       " 'rews': array([[-0.93239437, -0.51267606]]),\n",
       " 'lens': array([356]),\n",
       " 'idxs': array([0]),\n",
       " 'rew': -0.722535211267605,\n",
       " 'len': 356.0,\n",
       " 'rew_std': 0.20985915492957719,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed()\n",
    "seed=15\n",
    "human_collector.reset_env(gym_reset_kwargs={\"seed\":seed})\n",
    "human_collector.collect(n_episode=1, gym_reset_kwargs={\"seed\":seed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = MazeEnv_v0.MazeEnv(render_mode='human', size=6)\n",
    "maze.reset(seed=1, options={\"maze_type\":\"trivial\"})\n",
    "maze.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze.step(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "# Trivial Maze Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of mazes: 1\n",
      "Evaluation Reward at Epoch 1. Obs: -0.996\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.998\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.996\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.988\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.988\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.988\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 7. Obs: 0.986\n",
      "Test Mazes results: [1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 8. Obs: -0.989\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 9. Obs: -0.996\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 10. Obs: -0.995\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 11. Obs: 0.986\n",
      "Test Mazes results: [1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 12. Obs: -0.994\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 13. Obs: -0.998\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 14. Obs: -0.998\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 15. Obs: -0.998\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 16. Obs: -0.998\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 17. Obs: -0.998\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 18. Obs: -0.993\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 19. Obs: 0.986\n",
      "Test Mazes results: [1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 20. Obs: 0.986\n",
      "Test Mazes results: [1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 2.\n",
      "Solved all mazes on epoch 20.\n",
      "Current number of mazes: 2\n",
      "Evaluation Reward at Epoch 1. Obs: -0.988\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 2. Obs: 0.984\n",
      "Test Mazes results: [1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 3. Obs: 0.984\n",
      "Test Mazes results: [1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 3.\n",
      "Solved all mazes on epoch 3.\n",
      "Current number of mazes: 3\n",
      "Evaluation Reward at Epoch 1. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.999\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.993\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.993\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -0.994\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: -0.994\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 10. Obs: -0.994\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 11. Obs: -0.999\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 12. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 13. Obs: -0.998\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 14. Obs: -0.994\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 15. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 16. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 4.\n",
      "Solved all mazes on epoch 16.\n",
      "Current number of mazes: 4\n",
      "Evaluation Reward at Epoch 1. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 4. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 5.\n",
      "Solved all mazes on epoch 4.\n",
      "Current number of mazes: 5\n",
      "Evaluation Reward at Epoch 1. Obs: 0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 2. Obs: 0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 6.\n",
      "Solved all mazes on epoch 2.\n",
      "Current number of mazes: 6\n",
      "Evaluation Reward at Epoch 1. Obs: -0.999\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.994\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.999\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.997\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 7. Obs: -0.998\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 8. Obs: -0.993\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: -0.995\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 10. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 11. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 7.\n",
      "Solved all mazes on epoch 11.\n",
      "Current number of mazes: 7\n",
      "Evaluation Reward at Epoch 1. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.993\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.997\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.997\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 6. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 7. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 8.\n",
      "Solved all mazes on epoch 7.\n",
      "Current number of mazes: 8\n",
      "Evaluation Reward at Epoch 1. Obs: 0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 2. Obs: 0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 9.\n",
      "Solved all mazes on epoch 2.\n",
      "Current number of mazes: 9\n",
      "Evaluation Reward at Epoch 1. Obs: -1.0\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.998\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.998\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 7. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 8. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 10.\n",
      "Solved all mazes on epoch 8.\n",
      "Current number of mazes: 10\n",
      "Evaluation Reward at Epoch 1. Obs: -0.99\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: 0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 3. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: 0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 6. Obs: 0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 11.\n",
      "Solved all mazes on epoch 6.\n",
      "Current number of mazes: 11\n",
      "Evaluation Reward at Epoch 1. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 4. Obs: -0.999\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 8. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 12.\n",
      "Solved all mazes on epoch 8.\n",
      "Current number of mazes: 12\n",
      "Evaluation Reward at Epoch 1. Obs: -0.99\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -1.0\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 4. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 5. Obs: -0.997\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 8. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 13.\n",
      "Solved all mazes on epoch 8.\n",
      "Current number of mazes: 13\n",
      "Evaluation Reward at Epoch 1. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 2. Obs: -0.994\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.996\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 7. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 14.\n",
      "Solved all mazes on epoch 7.\n",
      "Current number of mazes: 14\n",
      "Evaluation Reward at Epoch 1. Obs: -0.996\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 4. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 15.\n",
      "Solved all mazes on epoch 4.\n",
      "Current number of mazes: 15\n",
      "Evaluation Reward at Epoch 1. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.997\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.996\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.992\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 8. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: -0.997\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 10. Obs: -0.999\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 11. Obs: -0.998\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 12. Obs: -0.997\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 13. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 14. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 16.\n",
      "Solved all mazes on epoch 14.\n",
      "Current number of mazes: 16\n",
      "Evaluation Reward at Epoch 1. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 2. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 4. Obs: -0.995\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 7. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 17.\n",
      "Solved all mazes on epoch 7.\n",
      "Current number of mazes: 17\n",
      "Evaluation Reward at Epoch 1. Obs: -0.997\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 3. Obs: -0.993\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 5. Obs: -0.992\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 7. Obs: -0.992\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 9. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 18.\n",
      "Solved all mazes on epoch 9.\n",
      "Current number of mazes: 18\n",
      "Evaluation Reward at Epoch 1. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 2. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.993\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 6. Obs: -0.994\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 8. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 19.\n",
      "Solved all mazes on epoch 8.\n",
      "Current number of mazes: 19\n",
      "Evaluation Reward at Epoch 1. Obs: -0.993\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.997\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: 0.983\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 4. Obs: 0.983\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 20.\n",
      "Solved all mazes on epoch 4.\n",
      "Current number of mazes: 20\n",
      "Evaluation Reward at Epoch 1. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 2. Obs: -0.998\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.999\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -0.996\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 9. Obs: -0.99\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 10. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 11. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 12. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 13. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 14. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 21.\n",
      "Solved all mazes on epoch 14.\n",
      "Current number of mazes: 21\n",
      "Evaluation Reward at Epoch 1. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.995\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.993\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -0.993\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 9. Obs: -0.994\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 10. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 11. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 22.\n",
      "Solved all mazes on epoch 11.\n",
      "Current number of mazes: 22\n",
      "Evaluation Reward at Epoch 1. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: 0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 3. Obs: -0.99\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: 0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 6. Obs: 0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 23.\n",
      "Solved all mazes on epoch 6.\n",
      "Current number of mazes: 23\n",
      "Evaluation Reward at Epoch 1. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.996\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 5. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 24.\n",
      "Solved all mazes on epoch 5.\n",
      "Current number of mazes: 24\n",
      "Evaluation Reward at Epoch 1. Obs: -0.998\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 3. Obs: -0.995\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.0\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.99\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -1.0\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -1.0\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 10. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 25.\n",
      "Solved all mazes on epoch 10.\n",
      "Current number of mazes: 25\n",
      "Evaluation Reward at Epoch 1. Obs: -0.999\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 8. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 26.\n",
      "Solved all mazes on epoch 8.\n",
      "Current number of mazes: 26\n",
      "Evaluation Reward at Epoch 1. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 2. Obs: 0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 27.\n",
      "Solved all mazes on epoch 2.\n",
      "Current number of mazes: 27\n",
      "Evaluation Reward at Epoch 1. Obs: -0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.996\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 8. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 10. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 28.\n",
      "Solved all mazes on epoch 10.\n",
      "Current number of mazes: 28\n",
      "Evaluation Reward at Epoch 1. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 4. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 29.\n",
      "Solved all mazes on epoch 4.\n",
      "Current number of mazes: 29\n",
      "Evaluation Reward at Epoch 1. Obs: -0.995\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: -0.997\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 10. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 11. Obs: -0.999\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 12. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 13. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 14. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 15. Obs: -0.994\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 16. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 17. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 18. Obs: -0.996\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 19. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 20. Obs: -0.992\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 21. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 22. Obs: -0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 23. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 24. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 25. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 26. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 30.\n",
      "Solved all mazes on epoch 26.\n",
      "Current number of mazes: 30\n",
      "Evaluation Reward at Epoch 1. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 2. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.999\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.0\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.997\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -0.996\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: -0.987\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 10. Obs: 0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 11. Obs: 0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 31.\n",
      "Solved all mazes on epoch 11.\n",
      "Current number of mazes: 31\n",
      "Evaluation Reward at Epoch 1. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 4. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 6. Obs: -0.993\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 8. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 32.\n",
      "Solved all mazes on epoch 8.\n",
      "Current number of mazes: 32\n",
      "Evaluation Reward at Epoch 1. Obs: -0.993\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.993\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 5. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -1.0\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 10. Obs: 0.986\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 33.\n",
      "Solved all mazes on epoch 10.\n",
      "Current number of mazes: 33\n",
      "Evaluation Reward at Epoch 1. Obs: -0.985\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.993\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: 0.988\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 5. Obs: -0.999\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.998\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 9. Obs: 0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 34.\n",
      "Solved all mazes on epoch 9.\n",
      "Current number of mazes: 34\n",
      "Evaluation Reward at Epoch 1. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.998\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 6. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 35.\n",
      "Solved all mazes on epoch 6.\n",
      "Current number of mazes: 35\n",
      "Evaluation Reward at Epoch 1. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.991\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.998\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 7. Obs: -0.998\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 8. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 9. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 10. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 11. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 12. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 13. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 14. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 15. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 36.\n",
      "Solved all mazes on epoch 15.\n",
      "Current number of mazes: 36\n",
      "Evaluation Reward at Epoch 1. Obs: -1.0\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.996\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.995\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.989\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.992\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -0.999\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 9. Obs: 0.984\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Current number of mazes: 37.\n",
      "Solved all mazes on epoch 9.\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "# manual training loop\n",
    "np.random.seed()\n",
    "\n",
    "\"\"\"\n",
    "# collect a bunch of random ones\n",
    "policy.policies[agents[0]].set_eps(1)\n",
    "policy.policies[agents[1]].set_eps(1)\n",
    "train_collector.collect(n_step=10000)\n",
    "\"\"\"\n",
    "for mazes in range(1, total_mazes+1):\n",
    "    if passed_mazes == True:\n",
    "        # reset epsilon again for the new maze\n",
    "        eps_train = 0.9\n",
    "        passed_mazes = False\n",
    "        print(f\"Current number of mazes: {mazes}\")\n",
    "    else:\n",
    "        print(\"Failed to find a solution within suitable time. Stopping training.\")\n",
    "        break\n",
    "\n",
    "    # set first eps\n",
    "    set_eps(eps_train, single=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # reset number of steps couhnt\n",
    "        steps_n = 0 \n",
    "        \n",
    "        # training loop\n",
    "        while steps_n < step_per_epoch:\n",
    "            # have runs where the exploration rate is very high\n",
    "            if np.random.randint(0, 10) == 0:\n",
    "                eps_prev = eps_train\n",
    "                eps_train = 0.9\n",
    "                high_eps_run = True\n",
    "            \n",
    "            set_eps(eps_train, single=True)\n",
    "\n",
    "            # train the model in training environment\n",
    "            train_collector.reset_env(gym_reset_kwargs={\"options\":{\"maze_type\":\"trivial\", \"n_mazes\":mazes, \"random\":True}})\n",
    "            result = train_collector.collect(n_episode=ep_per_collect, gym_reset_kwargs={\"options\":{\"maze_type\":\"trivial\", \"n_mazes\":mazes, \"random\":True}})\n",
    "            steps_n += int(result['n/st'])\n",
    "            steps_total += int(result['n/st'])\n",
    "            episodes_total += int(result['n/ep'])\n",
    "            \n",
    "            # update the parameters after train_num steps\n",
    "            policy.update(batch_size, train_collector.buffer)\n",
    "\n",
    "            #  reset high exploration\n",
    "            if high_eps_run:\n",
    "                eps_train = eps_prev\n",
    "                high_eps_run = False\n",
    "\n",
    "            # set the random training epsilon after each steps per collect\n",
    "            # decay it by specified parameter every\n",
    "            eps_train *= eps_decay\n",
    "            eps_train = np.max([eps_train, eps_min])\n",
    "            \n",
    "            # log\n",
    "            if result[\"n/ep\"] > 0:\n",
    "                log_data = {\"train\":{\n",
    "                        \"episode\": result[\"n/ep\"],\n",
    "                        \"obs_reward\": np.mean(result[\"rews\"]),\n",
    "                        \"length\": result[\"len\"],\n",
    "                        \"exploration rate\": eps_train,\n",
    "                        \"episodes\": episodes_total\n",
    "                    }\n",
    "                }\n",
    "                wandb.log(data=log_data, step=steps_total)\n",
    "        \n",
    "        # check test results\n",
    "        set_eps(eps_test, single=True)\n",
    "        policy.eval()\n",
    "        \n",
    "        passed_before = True\n",
    "        test_mazes = []\n",
    "        for seed in range(mazes):\n",
    "            # test through all previous mazes\n",
    "            test_collector.reset_env(gym_reset_kwargs={\"options\":{\"maze_type\":\"trivial\", \"n_mazes\":seed, \"random\":False}})\n",
    "            test_result = test_collector.collect(n_episode=test_num, gym_reset_kwargs={\"options\":{\"maze_type\":\"trivial\", \"n_mazes\":seed, \"random\":False}})\n",
    "            \n",
    "            # if any of the previous mazes failed, break out of loop and do the train loop again\n",
    "            if np.mean(test_result['rews']) < threshold_rew:\n",
    "                passed_mazes = False\n",
    "                passed_before = False\n",
    "                test_mazes.append(0)\n",
    "                break\n",
    "            else:\n",
    "                test_mazes.append(1)\n",
    "\n",
    "        # log\n",
    "        log_data = {\"test\":{\n",
    "                \"obs_reward\": np.mean(test_result[\"rews\"]),\n",
    "                \"length\": test_result[\"len\"],\n",
    "                \"obs_reward_std\": np.std(test_result[\"rews\"]),\n",
    "            }\n",
    "        }\n",
    "        wandb.log(data=log_data, step=steps_total)\n",
    "        \n",
    "        print(f\"Evaluation Reward at Epoch {epoch+1}. Obs: {np.round(np.mean(test_result['rews']), 3)}, End at Maze: {seed}\")\n",
    "        \n",
    "        print(f\"Test Mazes results: {test_mazes}\")\n",
    "\n",
    "        # every n epochs render the policy for human-based evalution\n",
    "        if (epoch % 10) == 0:\n",
    "            for seed in range(mazes):\n",
    "                watch({\"options\":{\"maze_type\":\"trivial\", \"n_mazes\":seed, \"random\":False}})\n",
    "            # reset back to training mode\n",
    "            policy.train()\n",
    "        \n",
    "        # reset back to training\n",
    "        set_eps(eps_train, single=True)\n",
    "        policy.train()\n",
    "\n",
    "        # check if the agent can auccessfully solve the maze (within some threshold)\n",
    "        if passed_mazes:\n",
    "            print(f\"Agents solved the current maze and all previous mazes. Current number of mazes: {mazes}.\")\n",
    "            print(f\"Solved all mazes on epoch {epoch+1}.\")\n",
    "            for seed in range(mazes):\n",
    "                watch({\"options\":{\"maze_type\":\"trivial\", \"n_mazes\":seed, \"random\":False}})\n",
    "            # reset back to training mode\n",
    "            policy.train()\n",
    "            break\n",
    "        \n",
    "        # to see if the agent can do it consequtively\n",
    "        if passed_before:\n",
    "            # pass the test maze at least twice\n",
    "            passed_mazes = True\n",
    "            print(\"Passed once\")\n",
    "        \n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-blaze-48</strong> at: <a href='https://wandb.ai/dandd0/Final_Year_Project/runs/1p1f66ws' target=\"_blank\">https://wandb.ai/dandd0/Final_Year_Project/runs/1p1f66ws</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230620_173107-1p1f66ws\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), \"model/trivial_maze_baseline_20-6-23.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Random Maze Single Agent Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of mazes: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Desktop\\University\\Exeter\\ECMM451 Final Year Project\\Final_Year_Project\\maze_env\\envs\\mazelib.py:32: DeprecationWarning: Seeding based on hashing is deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version. The only \n",
      "supported seed types are: None, int, float, str, bytes, and bytearray.\n",
      "  random.seed(seed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Reward at Epoch 1. Obs: 0.995, Maze: 1\n",
      "Test Mazes results: [1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 2. Obs: -1.001, Maze: 1\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.996, Maze: 1\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.001, Maze: 1\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.001, Maze: 1\n",
      "Test Mazes results: [0]\n",
      "Evaluation Reward at Epoch 6. Obs: 0.995, Maze: 1\n",
      "Test Mazes results: [1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 7. Obs: 0.995, Maze: 1\n",
      "Test Mazes results: [1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 7.\n",
      "Current number of mazes: 2\n",
      "Evaluation Reward at Epoch 1. Obs: -1.0, Maze: 2\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.999, Maze: 2\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -1.0, Maze: 2\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0, Maze: 2\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.999, Maze: 2\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.999, Maze: 2\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -1.001, Maze: 2\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -0.999, Maze: 2\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: -0.999, Maze: 2\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 10. Obs: -1.001, Maze: 2\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 11. Obs: -1.001, Maze: 2\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 12. Obs: 0.994, Maze: 2\n",
      "Test Mazes results: [1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 13. Obs: 0.994, Maze: 2\n",
      "Test Mazes results: [1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 13.\n",
      "Current number of mazes: 3\n",
      "Evaluation Reward at Epoch 1. Obs: -0.995, Maze: 2\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.999, Maze: 3\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.999, Maze: 3\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.999, Maze: 3\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.999, Maze: 3\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.998, Maze: 3\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -0.999, Maze: 3\n",
      "Test Mazes results: [1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: 0.995, Maze: 3\n",
      "Test Mazes results: [1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 9. Obs: 0.995, Maze: 3\n",
      "Test Mazes results: [1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 9.\n",
      "Current number of mazes: 4\n",
      "Evaluation Reward at Epoch 1. Obs: -0.999, Maze: 4\n",
      "Test Mazes results: [1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: 0.994, Maze: 4\n",
      "Test Mazes results: [1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 3. Obs: 0.994, Maze: 4\n",
      "Test Mazes results: [1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 3.\n",
      "Current number of mazes: 5\n",
      "Evaluation Reward at Epoch 1. Obs: 0.994, Maze: 5\n",
      "Test Mazes results: [1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 2. Obs: 0.994, Maze: 5\n",
      "Test Mazes results: [1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 2.\n",
      "Current number of mazes: 6\n",
      "Evaluation Reward at Epoch 1. Obs: -0.997, Maze: 2\n",
      "Test Mazes results: [1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: 0.992, Maze: 6\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 3. Obs: 0.992, Maze: 6\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 3.\n",
      "Current number of mazes: 7\n",
      "Evaluation Reward at Epoch 1. Obs: -0.998, Maze: 7\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.995, Maze: 4\n",
      "Test Mazes results: [1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.997, Maze: 7\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.993, Maze: 7\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -0.998, Maze: 7\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.998, Maze: 7\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -1.001, Maze: 7\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -1.001, Maze: 7\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: 0.991, Maze: 7\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 10. Obs: 0.991, Maze: 7\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 10.\n",
      "Current number of mazes: 8\n",
      "Evaluation Reward at Epoch 1. Obs: -0.999, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -1.0, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.001, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.999, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -1.001, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -1.001, Maze: 7\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: -0.999, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 10. Obs: -1.001, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 11. Obs: -1.001, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 12. Obs: -1.001, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 13. Obs: -1.001, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 14. Obs: -1.0, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 15. Obs: -0.996, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 16. Obs: -1.0, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 17. Obs: -1.0, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 18. Obs: 0.991, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 19. Obs: 0.991, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 19.\n",
      "Current number of mazes: 9\n",
      "Evaluation Reward at Epoch 1. Obs: -1.0, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -1.001, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.001, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -1.0, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -0.999, Maze: 4\n",
      "Test Mazes results: [1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -1.001, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: 0.989, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 10. Obs: 0.989, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 10.\n",
      "Current number of mazes: 10\n",
      "Evaluation Reward at Epoch 1. Obs: -1.001, Maze: 10\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.0, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.998, Maze: 10\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.001, Maze: 10\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: 0.991, Maze: 10\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 6. Obs: 0.991, Maze: 10\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 6.\n",
      "Current number of mazes: 11\n",
      "Evaluation Reward at Epoch 1. Obs: -0.996, Maze: 11\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.001, Maze: 11\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.998, Maze: 11\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.0, Maze: 11\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.001, Maze: 11\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -1.0, Maze: 11\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -0.999, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -1.0, Maze: 11\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: 0.988, Maze: 11\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 10. Obs: 0.988, Maze: 11\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 10.\n",
      "Current number of mazes: 12\n",
      "Evaluation Reward at Epoch 1. Obs: -0.999, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.999, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.999, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.996, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.001, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -1.0, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -1.001, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -0.994, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: -0.986, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 10. Obs: 0.988, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 11. Obs: 0.988, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 11.\n",
      "Current number of mazes: 13\n",
      "Evaluation Reward at Epoch 1. Obs: -0.999, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.999, Maze: 11\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -1.001, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -1.001, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.001, Maze: 13\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.997, Maze: 13\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -1.0, Maze: 13\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -1.001, Maze: 10\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: -1.001, Maze: 13\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 10. Obs: -1.001, Maze: 13\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 11. Obs: -1.001, Maze: 13\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 12. Obs: -0.999, Maze: 13\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 13. Obs: -0.999, Maze: 13\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 14. Obs: -1.001, Maze: 10\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 15. Obs: -0.999, Maze: 13\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 16. Obs: -1.001, Maze: 13\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 17. Obs: 0.987, Maze: 13\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 18. Obs: 0.987, Maze: 13\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 18.\n",
      "Current number of mazes: 14\n",
      "Evaluation Reward at Epoch 1. Obs: -1.001, Maze: 14\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -1.001, Maze: 14\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -1.0, Maze: 14\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.997, Maze: 14\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.001, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.996, Maze: 14\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -0.998, Maze: 14\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -1.001, Maze: 13\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: -1.001, Maze: 10\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 10. Obs: 0.987, Maze: 14\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 11. Obs: 0.987, Maze: 14\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 11.\n",
      "Current number of mazes: 15\n",
      "Evaluation Reward at Epoch 1. Obs: -1.0, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 2. Obs: -0.999, Maze: 11\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 3. Obs: -0.999, Maze: 9\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 4. Obs: -0.998, Maze: 11\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 5. Obs: -1.001, Maze: 12\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 6. Obs: -0.994, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 7. Obs: -0.996, Maze: 7\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 8. Obs: -0.999, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 9. Obs: -1.001, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 10. Obs: -1.001, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 11. Obs: -0.997, Maze: 14\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 12. Obs: -0.994, Maze: 8\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 13. Obs: -1.001, Maze: 10\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 14. Obs: -1.0, Maze: 14\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 15. Obs: -0.995, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 16. Obs: -1.001, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 17. Obs: -1.001, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 18. Obs: -0.999, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 19. Obs: -1.001, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 20. Obs: -1.0, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 21. Obs: -1.001, Maze: 10\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 22. Obs: -1.001, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 23. Obs: -0.998, Maze: 14\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 24. Obs: 0.987, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 25. Obs: -1.001, Maze: 10\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Evaluation Reward at Epoch 26. Obs: 0.987, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Passed once\n",
      "Evaluation Reward at Epoch 27. Obs: 0.987, Maze: 15\n",
      "Test Mazes results: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Agents solved the current maze and all previous mazes. Solved all mazes on epoch 27.\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "# manual training loop\n",
    "np.random.seed()\n",
    "\n",
    "\"\"\"\n",
    "# collect a bunch of random ones\n",
    "policy.policies[agents[0]].set_eps(1)\n",
    "policy.policies[agents[1]].set_eps(1)\n",
    "train_collector.collect(n_step=10000)\n",
    "\"\"\"\n",
    "for mazes in range(1, total_mazes):\n",
    "    if passed_mazes == True:\n",
    "        # reset epsilon again for the new maze\n",
    "        eps_train = eps_train_start\n",
    "        passed_mazes = False\n",
    "        print(f\"Current number of mazes: {mazes}\")\n",
    "    else:\n",
    "        print(\"Failed to find a solution within suitable time. Stopping training.\")\n",
    "        break\n",
    "\n",
    "    # reset\n",
    "    set_eps(eps_train, single=True)\n",
    "    policy.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # reset number of steps couhnt\n",
    "        steps_n = 0 \n",
    "        \n",
    "        # training loop\n",
    "        while steps_n < step_per_epoch:\n",
    "            # have runs where the exploration rate is very high\n",
    "            if np.random.randint(0, 10) == 0:\n",
    "                eps_prev = eps_train\n",
    "                eps_train = eps_train_start\n",
    "                high_eps_run = True\n",
    "            \n",
    "            set_eps(eps_train, single=True)\n",
    "\n",
    "            # train the model in training environment\n",
    "            train_collector.reset_env(gym_reset_kwargs={\"options\":{\"maze_type\":maze_type, \"n_mazes\":mazes, \"random\":True}})\n",
    "            result = train_collector.collect(n_episode=ep_per_collect, gym_reset_kwargs={\"options\":{\"maze_type\":maze_type, \"n_mazes\":mazes, \"random\":True}})\n",
    "            steps_n += int(result['n/st'])\n",
    "            steps_total += int(result['n/st'])\n",
    "            episodes_total += int(result['n/ep'])\n",
    "            \n",
    "            # update the parameters after train_num steps\n",
    "            policy.update(batch_size, train_collector.buffer)\n",
    "\n",
    "            #  reset high exploration\n",
    "            if high_eps_run:\n",
    "                eps_train = eps_prev\n",
    "                high_eps_run = False\n",
    "\n",
    "            # set the random training epsilon after each steps per collect\n",
    "            # decay it by specified parameter every\n",
    "            eps_train *= eps_decay\n",
    "            eps_train = np.max([eps_train, eps_min])\n",
    "            \n",
    "            # log\n",
    "            if result[\"n/ep\"] > 0:\n",
    "                log_data = {\"train\":{\n",
    "                        \"episode\": result[\"n/ep\"],\n",
    "                        \"obs_reward\": np.mean(result[\"rews\"]),\n",
    "                        \"length\": result[\"len\"],\n",
    "                        \"exploration rate\": eps_train,\n",
    "                        \"episodes\": episodes_total\n",
    "                    }\n",
    "                }\n",
    "                wandb.log(data=log_data, step=steps_total)\n",
    "        \n",
    "        # check test results\n",
    "        set_eps(eps_test, single=True)\n",
    "        policy.eval()\n",
    "        \n",
    "        passed_before = True\n",
    "        test_mazes = []\n",
    "        for seed in range(1, mazes+1):\n",
    "            # test through all previous mazes\n",
    "            test_collector.reset_env(gym_reset_kwargs={\"options\":{\"maze_type\":maze_type, \"n_mazes\":seed, \"random\":False}})\n",
    "            test_result = test_collector.collect(n_episode=test_num, gym_reset_kwargs={\"options\":{\"maze_type\":maze_type, \"n_mazes\":seed, \"random\":False}})\n",
    "            \n",
    "            # if any of the previous mazes failed, break out of loop and do the train loop again\n",
    "            if np.mean(test_result['rews']) < threshold_rew:\n",
    "                passed_mazes = False\n",
    "                passed_before = False\n",
    "                test_mazes.append(0)\n",
    "                break\n",
    "            else:\n",
    "                test_mazes.append(1)\n",
    "\n",
    "        # log\n",
    "        log_data = {\"test\":{\n",
    "                \"obs_reward\": np.mean(test_result[\"rews\"]),\n",
    "                \"length\": test_result[\"len\"],\n",
    "                \"obs_reward_std\": np.std(test_result[\"rews\"]),\n",
    "            }\n",
    "        }\n",
    "        wandb.log(data=log_data, step=steps_total)\n",
    "        \n",
    "        print(f\"Evaluation Reward at Epoch {epoch+1}. Obs: {np.round(np.mean(test_result['rews']), 3)}, Maze: {seed}\")\n",
    "        \n",
    "        print(f\"Test Mazes results: {test_mazes}\")\n",
    "\n",
    "        # every n epochs render the policy for human-based evalution\n",
    "        if (epoch % 10) == 0:\n",
    "            for seed in range(1, mazes+1):\n",
    "                watch({\"options\":{\"maze_type\":maze_type, \"n_mazes\":seed, \"random\":False}})\n",
    "            # reset back to training mode\n",
    "            policy.train()\n",
    "\n",
    "        # check if the agent can auccessfully solve the maze (within some threshold)\n",
    "        if passed_mazes:\n",
    "            print(f\"Agents solved the current maze and all previous mazes. Solved all mazes on epoch {epoch+1}.\")\n",
    "\n",
    "            # watch the results if successful in passing twice\n",
    "            for seed in range(1, mazes+1):\n",
    "                watch({\"options\":{\"maze_type\":maze_type, \"n_mazes\":seed, \"random\":False}})\n",
    "            # reset back to training mode\n",
    "            policy.train()\n",
    "            break\n",
    "        \n",
    "        # to see if the agent can do it consequtively\n",
    "        if passed_before:\n",
    "            # pass the test maze at least twice\n",
    "            passed_mazes = True\n",
    "            print(\"Passed once\")\n",
    "        \n",
    "        # reset back to training\n",
    "        set_eps(eps_train, single=True)\n",
    "        policy.train()\n",
    "        \n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dickson Lim\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\ipython.py:70: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-capybara-56</strong> at: <a href='https://wandb.ai/dandd0/Final_Year_Project/runs/o3tvr73u' target=\"_blank\">https://wandb.ai/dandd0/Final_Year_Project/runs/o3tvr73u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230626_152213-o3tvr73u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mazes = 1\n",
    "policy.set_eps(1)\n",
    "\n",
    "train_collector.reset_env(gym_reset_kwargs={\"options\":{\"maze_type\":\"random\", \"n_mazes\":mazes, \"random\":True}})\n",
    "result = train_collector.collect(n_episode=ep_per_collect, gym_reset_kwargs={\"options\":{\"maze_type\":\"random\", \"n_mazes\":mazes, \"random\":True}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 1,\n",
       " 'n/st': 711,\n",
       " 'rews': array([[-0.99225352]]),\n",
       " 'lens': array([711]),\n",
       " 'idxs': array([194]),\n",
       " 'rew': -0.9922535211267594,\n",
       " 'len': 711.0,\n",
       " 'rew_std': 0.0,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorReplayBuffer(\n",
       "    info: Batch(\n",
       "              env_id: array([0, 0, 0, ..., 0, 0, 0]),\n",
       "          ),\n",
       "    policy: Batch(),\n",
       "    obs: Batch(\n",
       "             agent_id: array(['observer', 'observer', 'observer', ..., None, None, None],\n",
       "                             dtype=object),\n",
       "             obs: array([[[[1., 1., 1., ..., 1., 1., 1.],\n",
       "                           [1., 0., 0., ..., 1., 0., 1.],\n",
       "                           [1., 1., 1., ..., 1., 0., 1.],\n",
       "                           ...,\n",
       "                           [1., 0., 1., ..., 1., 0., 1.],\n",
       "                           [1., 0., 0., ..., 0., 0., 1.],\n",
       "                           [1., 1., 1., ..., 1., 1., 1.]],\n",
       "                  \n",
       "                          [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                  \n",
       "                          [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "                  \n",
       "                  \n",
       "                         [[[1., 1., 1., ..., 1., 1., 1.],\n",
       "                           [1., 0., 0., ..., 1., 0., 1.],\n",
       "                           [1., 1., 1., ..., 1., 0., 1.],\n",
       "                           ...,\n",
       "                           [1., 0., 1., ..., 1., 0., 1.],\n",
       "                           [1., 0., 0., ..., 0., 0., 1.],\n",
       "                           [1., 1., 1., ..., 1., 1., 1.]],\n",
       "                  \n",
       "                          [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                  \n",
       "                          [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "                  \n",
       "                  \n",
       "                         [[[1., 1., 1., ..., 1., 1., 1.],\n",
       "                           [1., 0., 0., ..., 1., 0., 1.],\n",
       "                           [1., 1., 1., ..., 1., 0., 1.],\n",
       "                           ...,\n",
       "                           [1., 0., 1., ..., 1., 0., 1.],\n",
       "                           [1., 0., 0., ..., 0., 0., 1.],\n",
       "                           [1., 1., 1., ..., 1., 1., 1.]],\n",
       "                  \n",
       "                          [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                  \n",
       "                          [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "                  \n",
       "                  \n",
       "                         ...,\n",
       "                  \n",
       "                  \n",
       "                         [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                  \n",
       "                          [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                  \n",
       "                          [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "                  \n",
       "                  \n",
       "                         [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                  \n",
       "                          [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                  \n",
       "                          [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "                  \n",
       "                  \n",
       "                         [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                  \n",
       "                          [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                  \n",
       "                          [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           ...,\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.],\n",
       "                           [0., 0., 0., ..., 0., 0., 0.]]]]),\n",
       "             mask: array([[False, False,  True,  True, False],\n",
       "                          [False,  True,  True, False, False],\n",
       "                          [False, False,  True,  True, False],\n",
       "                          ...,\n",
       "                          [False, False, False, False, False],\n",
       "                          [False, False, False, False, False],\n",
       "                          [False, False, False, False, False]]),\n",
       "         ),\n",
       "    obs_next: Batch(\n",
       "                  agent_id: array(['observer', 'observer', 'observer', ..., None, None, None],\n",
       "                                  dtype=object),\n",
       "                  obs: array([[[[1., 1., 1., ..., 1., 1., 1.],\n",
       "                                [1., 0., 0., ..., 1., 0., 1.],\n",
       "                                [1., 1., 1., ..., 1., 0., 1.],\n",
       "                                ...,\n",
       "                                [1., 0., 1., ..., 1., 0., 1.],\n",
       "                                [1., 0., 0., ..., 0., 0., 1.],\n",
       "                                [1., 1., 1., ..., 1., 1., 1.]],\n",
       "                       \n",
       "                               [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                       \n",
       "                               [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "                       \n",
       "                       \n",
       "                              [[[1., 1., 1., ..., 1., 1., 1.],\n",
       "                                [1., 0., 0., ..., 1., 0., 1.],\n",
       "                                [1., 1., 1., ..., 1., 0., 1.],\n",
       "                                ...,\n",
       "                                [1., 0., 1., ..., 1., 0., 1.],\n",
       "                                [1., 0., 0., ..., 0., 0., 1.],\n",
       "                                [1., 1., 1., ..., 1., 1., 1.]],\n",
       "                       \n",
       "                               [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                       \n",
       "                               [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "                       \n",
       "                       \n",
       "                              [[[1., 1., 1., ..., 1., 1., 1.],\n",
       "                                [1., 0., 0., ..., 1., 0., 1.],\n",
       "                                [1., 1., 1., ..., 1., 0., 1.],\n",
       "                                ...,\n",
       "                                [1., 0., 1., ..., 1., 0., 1.],\n",
       "                                [1., 0., 0., ..., 0., 0., 1.],\n",
       "                                [1., 1., 1., ..., 1., 1., 1.]],\n",
       "                       \n",
       "                               [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                       \n",
       "                               [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "                       \n",
       "                       \n",
       "                              ...,\n",
       "                       \n",
       "                       \n",
       "                              [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                       \n",
       "                               [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                       \n",
       "                               [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "                       \n",
       "                       \n",
       "                              [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                       \n",
       "                               [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                       \n",
       "                               [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "                       \n",
       "                       \n",
       "                              [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                       \n",
       "                               [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]],\n",
       "                       \n",
       "                               [[0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                ...,\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.],\n",
       "                                [0., 0., 0., ..., 0., 0., 0.]]]]),\n",
       "                  mask: array([[False,  True,  True, False, False],\n",
       "                               [False, False,  True,  True, False],\n",
       "                               [False,  True,  True, False, False],\n",
       "                               ...,\n",
       "                               [False, False, False, False, False],\n",
       "                               [False, False, False, False, False],\n",
       "                               [False, False, False, False, False]]),\n",
       "              ),\n",
       "    truncated: array([False, False, False, ..., False, False, False]),\n",
       "    terminated: array([False, False, False, ..., False, False, False]),\n",
       "    rew: array([[-0.00070423],\n",
       "                [-0.00140845],\n",
       "                [-0.00140845],\n",
       "                ...,\n",
       "                [ 0.        ],\n",
       "                [ 0.        ],\n",
       "                [ 0.        ]]),\n",
       "    act: array([2, 1, 2, ..., 0, 0, 0], dtype=int64),\n",
       "    done: array([False, False, False, ..., False, False, False]),\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_collector.buffer.get()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
