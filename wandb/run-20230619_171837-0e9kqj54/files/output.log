f:\Desktop\University\Exeter\ECMM451 Final Year Project\Final_Year_Project\maze_env\envs\mazelib.py:32: DeprecationWarning: Seeding based on hashing is deprecated
since Python 3.9 and will be removed in a subsequent version. The only
supported seed types are: None, int, float, str, bytes, and bytearray.
  random.seed(seed)
f:\Desktop\University\Exeter\ECMM451 Final Year Project\Final_Year_Project\maze_env\envs\mazelib.py:32: DeprecationWarning: Seeding based on hashing is deprecated
since Python 3.9 and will be removed in a subsequent version. The only
supported seed types are: None, int, float, str, bytes, and bytearray.
  random.seed(seed)
f:\Desktop\University\Exeter\ECMM451 Final Year Project\Final_Year_Project\maze_env\envs\mazelib.py:32: DeprecationWarning: Seeding based on hashing is deprecated
since Python 3.9 and will be removed in a subsequent version. The only
supported seed types are: None, int, float, str, bytes, and bytearray.
  random.seed(seed)
Evaluation Reward at Epoch 1. Obs: -350.6, Exp: -88.2
Test Mazes results: [1, 0, 0]
{"CNN": "type", "F": "module", "MazeEnv_v0": "module", "Net": "type", "PettingZooEnv": "ABCMeta", "PettingZooEnv_new": "ABCMeta", "SummaryWriter": "type", "action_shape": "int", "agent_explorer": "DQNPolicy", "agent_observer": "DQNPolicy", "agent_policies": "list", "agents": "list", "batch_size": "int", "buffer_size": "int", "env": "PettingZooEnv_new", "env_human": "DummyVectorEnv", "epoch": "int", "epochs": "int", "eps_decay": "float", "eps_min": "float", "eps_prev": "float64", "eps_test": "float", "eps_train": "float64", "gamma": "float", "gradient_n": "int", "gym": "module", "high_eps_run": "bool", "human_collector": "Collector", "interleave_training": "function", "log_data": "dict", "lr": "float", "maze": "int", "maze_width": "int", "mazes": "int", "n_mazes": "int", "n_step": "int", "net_exp": "Net", "net_obs": "CNN", "nn": "module", "np": "module", "obs_train": "bool", "optim_exp": "Adam", "optim_obs": "Adam", "passed_mazes": "bool", "policy": "MultiAgentPolicyManager", "preprocess_maze_env": "function", "result": "dict", "run": "Run", "seed": "int", "state_shape": "tuple", "step_per_collect": "int", "step_per_epoch": "int", "steps_n": "int", "steps_total": "int", "supersuit": "module", "target_update_freq": "int", "test_collector": "Collector", "test_envs": "DummyVectorEnv", "test_mazes": "list", "test_num": "int", "test_result": "dict", "torch": "module", "total_mazes": "int", "train_collector": "Collector", "train_envs": "DummyVectorEnv", "train_num": "int", "ts": "module", "wandb": "module"}
{"shape": "", "count": 1, "type": "dict"}
Evaluation Reward at Epoch 1. Obs: -350.2, Exp: -89.2
Test Mazes results: [1, 0, 0]
Evaluation Reward at Epoch 2. Obs: -350.7, Exp: -89.9
Evaluation Reward at Epoch 3. Obs: -350.7, Exp: -89.0
Evaluation Reward at Epoch 4. Obs: -350.6, Exp: -89.6
Evaluation Reward at Epoch 5. Obs: -350.8, Exp: -90.2
Evaluation Reward at Epoch 6. Obs: -350.7, Exp: -90.1
Evaluation Reward at Epoch 7. Obs: -350.8, Exp: -89.4
Evaluation Reward at Epoch 8. Obs: -350.5, Exp: -89.5
Evaluation Reward at Epoch 9. Obs: -350.5, Exp: -2.4
Evaluation Reward at Epoch 10. Obs: -350.6, Exp: -87.9
Evaluation Reward at Epoch 11. Obs: -350.7, Exp: -89.1
Test Mazes results: [1, 0, 0]
Evaluation Reward at Epoch 12. Obs: -350.7, Exp: -88.9
Evaluation Reward at Epoch 13. Obs: -350.4, Exp: -88.3
Evaluation Reward at Epoch 14. Obs: -350.7, Exp: -89.0
Evaluation Reward at Epoch 15. Obs: -350.7, Exp: -88.3
Evaluation Reward at Epoch 16. Obs: -350.9, Exp: -89.4
Evaluation Reward at Epoch 17. Obs: -350.7, Exp: -88.3
Evaluation Reward at Epoch 18. Obs: -350.8, Exp: -87.7
Evaluation Reward at Epoch 19. Obs: -350.4, Exp: -88.6
Evaluation Reward at Epoch 20. Obs: -350.7, Exp: -88.7
Evaluation Reward at Epoch 21. Obs: -350.7, Exp: -88.7
Test Mazes results: [1, 0, 0]
Evaluation Reward at Epoch 22. Obs: -350.2, Exp: -88.2
Evaluation Reward at Epoch 23. Obs: -350.6, Exp: -89.1
Evaluation Reward at Epoch 24. Obs: -351.3, Exp: -157.4
Evaluation Reward at Epoch 25. Obs: -350.9, Exp: -160.2
Evaluation Reward at Epoch 26. Obs: -350.9, Exp: -91.0
Evaluation Reward at Epoch 27. Obs: -350.7, Exp: -90.5
Evaluation Reward at Epoch 28. Obs: -351.0, Exp: -91.6
Evaluation Reward at Epoch 29. Obs: -350.6, Exp: -90.5
Evaluation Reward at Epoch 30. Obs: -350.8, Exp: -90.7
Evaluation Reward at Epoch 31. Obs: -351.6, Exp: -91.1
Test Mazes results: [0, 0, 0]
Evaluation Reward at Epoch 32. Obs: -350.9, Exp: -74.3
Evaluation Reward at Epoch 33. Obs: -352.2, Exp: -88.1
Evaluation Reward at Epoch 34. Obs: -351.0, Exp: -86.4
Evaluation Reward at Epoch 35. Obs: -351.6, Exp: -86.7
Evaluation Reward at Epoch 36. Obs: -351.5, Exp: -86.4
Evaluation Reward at Epoch 37. Obs: -351.7, Exp: -1.7
Evaluation Reward at Epoch 38. Obs: -351.4, Exp: -1.8
Evaluation Reward at Epoch 39. Obs: -351.6, Exp: -2.4
Evaluation Reward at Epoch 40. Obs: -351.4, Exp: -89.0
Evaluation Reward at Epoch 41. Obs: -351.3, Exp: -88.9
Test Mazes results: [1, 0, 0]
Evaluation Reward at Epoch 42. Obs: -351.6, Exp: -86.3
Evaluation Reward at Epoch 43. Obs: -351.1, Exp: -87.1
Evaluation Reward at Epoch 44. Obs: -350.5, Exp: -87.7
Evaluation Reward at Epoch 45. Obs: -351.7, Exp: -1.7
Evaluation Reward at Epoch 46. Obs: -351.3, Exp: -86.4
Evaluation Reward at Epoch 47. Obs: -351.4, Exp: -86.3
Evaluation Reward at Epoch 48. Obs: -351.4, Exp: -86.4
Evaluation Reward at Epoch 49. Obs: -351.7, Exp: -86.9
Evaluation Reward at Epoch 50. Obs: -351.6, Exp: -89.2
Evaluation Reward at Epoch 51. Obs: -351.7, Exp: -86.5
Test Mazes results: [0, 0, 0]
Evaluation Reward at Epoch 52. Obs: -351.9, Exp: -87.4
Evaluation Reward at Epoch 53. Obs: -351.9, Exp: -1.4
Evaluation Reward at Epoch 54. Obs: -351.5, Exp: -2.4
Evaluation Reward at Epoch 55. Obs: -351.8, Exp: -86.6
Evaluation Reward at Epoch 56. Obs: -351.5, Exp: -86.1
Evaluation Reward at Epoch 57. Obs: -351.4, Exp: -89.3
Evaluation Reward at Epoch 58. Obs: -351.5, Exp: -86.7
Evaluation Reward at Epoch 59. Obs: -351.5, Exp: -87.5
Evaluation Reward at Epoch 60. Obs: -350.7, Exp: -87.6
Evaluation Reward at Epoch 61. Obs: -351.8, Exp: -1.8
Test Mazes results: [0, 0, 0]
Evaluation Reward at Epoch 62. Obs: -351.6, Exp: -86.9
Evaluation Reward at Epoch 63. Obs: -352.1, Exp: -85.9
Evaluation Reward at Epoch 64. Obs: -351.8, Exp: -70.4
Evaluation Reward at Epoch 65. Obs: -351.8, Exp: -87.2
Evaluation Reward at Epoch 66. Obs: -351.1, Exp: -77.0
Evaluation Reward at Epoch 67. Obs: -351.6, Exp: -86.9
Evaluation Reward at Epoch 68. Obs: -351.2, Exp: -88.3
Evaluation Reward at Epoch 69. Obs: -351.4, Exp: -75.0
Evaluation Reward at Epoch 70. Obs: -351.5, Exp: -86.1
Evaluation Reward at Epoch 71. Obs: -351.8, Exp: -86.5
Test Mazes results: [0, 0, 0]
Evaluation Reward at Epoch 72. Obs: -351.5, Exp: -87.0
Evaluation Reward at Epoch 73. Obs: -350.6, Exp: -87.4
Evaluation Reward at Epoch 74. Obs: -351.5, Exp: -86.2
Evaluation Reward at Epoch 75. Obs: -351.4, Exp: -86.0
Evaluation Reward at Epoch 76. Obs: -351.9, Exp: -74.7
Evaluation Reward at Epoch 77. Obs: -352.4, Exp: -73.3
Evaluation Reward at Epoch 78. Obs: -351.3, Exp: -78.2
Evaluation Reward at Epoch 79. Obs: -350.8, Exp: -71.6
Evaluation Reward at Epoch 80. Obs: -351.4, Exp: -86.4
Evaluation Reward at Epoch 81. Obs: -351.3, Exp: -76.2
Test Mazes results: [0, 0, 0]
Evaluation Reward at Epoch 82. Obs: -351.3, Exp: -82.1
Evaluation Reward at Epoch 83. Obs: -351.5, Exp: -77.6
Evaluation Reward at Epoch 84. Obs: -351.8, Exp: -81.6
Evaluation Reward at Epoch 85. Obs: -351.0, Exp: -68.8
Evaluation Reward at Epoch 86. Obs: -351.1, Exp: -70.5
Evaluation Reward at Epoch 87. Obs: -351.7, Exp: -86.4
Evaluation Reward at Epoch 88. Obs: -351.6, Exp: -87.1
Evaluation Reward at Epoch 89. Obs: -351.7, Exp: -80.1
Evaluation Reward at Epoch 90. Obs: -351.5, Exp: -86.8
Evaluation Reward at Epoch 91. Obs: -351.6, Exp: -86.5
Test Mazes results: [0, 0, 0]
Evaluation Reward at Epoch 92. Obs: -351.0, Exp: -88.3
Evaluation Reward at Epoch 93. Obs: -351.6, Exp: -86.9
Evaluation Reward at Epoch 94. Obs: -351.8, Exp: -88.1
Evaluation Reward at Epoch 95. Obs: -351.0, Exp: -88.0
Evaluation Reward at Epoch 96. Obs: -351.5, Exp: -86.4
Evaluation Reward at Epoch 97. Obs: -351.3, Exp: -80.4
Evaluation Reward at Epoch 98. Obs: -351.9, Exp: -86.6
Evaluation Reward at Epoch 99. Obs: -351.7, Exp: -86.8
Evaluation Reward at Epoch 100. Obs: -351.5, Exp: -84.2
Evaluation Reward at Epoch 101. Obs: -351.7, Exp: -86.6
Test Mazes results: [0, 0, 0]
Evaluation Reward at Epoch 102. Obs: -351.4, Exp: -78.6
Evaluation Reward at Epoch 103. Obs: -351.3, Exp: -84.5
Evaluation Reward at Epoch 104. Obs: -351.3, Exp: -76.4
Evaluation Reward at Epoch 105. Obs: -351.6, Exp: -86.2
Evaluation Reward at Epoch 106. Obs: -351.8, Exp: -87.1
Evaluation Reward at Epoch 107. Obs: -351.6, Exp: -86.2
Evaluation Reward at Epoch 108. Obs: -350.7, Exp: -62.1
Evaluation Reward at Epoch 109. Obs: -351.9, Exp: -87.0
Evaluation Reward at Epoch 110. Obs: -351.8, Exp: -86.9
Evaluation Reward at Epoch 111. Obs: -351.4, Exp: -86.0
Test Mazes results: [0, 0, 0]
Evaluation Reward at Epoch 112. Obs: -351.4, Exp: -86.6
Evaluation Reward at Epoch 113. Obs: -350.5, Exp: -0.8
Evaluation Reward at Epoch 114. Obs: -350.4, Exp: -0.9
Evaluation Reward at Epoch 115. Obs: -352.6, Exp: -0.8
Evaluation Reward at Epoch 116. Obs: -350.4, Exp: -0.9
Evaluation Reward at Epoch 117. Obs: -351.3, Exp: -89.4
Evaluation Reward at Epoch 118. Obs: -350.3, Exp: -1.1
Evaluation Reward at Epoch 119. Obs: -350.5, Exp: -1.9
Evaluation Reward at Epoch 120. Obs: -350.7, Exp: -2.4
Evaluation Reward at Epoch 121. Obs: -350.8, Exp: -1.3
Test Mazes results: [0, 0, 0]
Evaluation Reward at Epoch 122. Obs: -351.6, Exp: -87.1
Evaluation Reward at Epoch 123. Obs: -351.6, Exp: -86.7
Evaluation Reward at Epoch 124. Obs: -351.8, Exp: -86.9
Evaluation Reward at Epoch 125. Obs: -351.7, Exp: -86.8
Evaluation Reward at Epoch 126. Obs: -350.7, Exp: -1.7
Evaluation Reward at Epoch 127. Obs: -350.7, Exp: -0.8
Evaluation Reward at Epoch 128. Obs: -350.7, Exp: -1.2
Evaluation Reward at Epoch 129. Obs: -351.8, Exp: -88.2
Evaluation Reward at Epoch 130. Obs: -351.7, Exp: -87.1
Evaluation Reward at Epoch 131. Obs: -351.7, Exp: -86.5
Test Mazes results: [0, 0, 0]
Evaluation Reward at Epoch 132. Obs: -351.5, Exp: -86.4
Evaluation Reward at Epoch 133. Obs: -351.5, Exp: -86.2
Evaluation Reward at Epoch 134. Obs: -351.8, Exp: -86.8
Evaluation Reward at Epoch 135. Obs: -351.9, Exp: -86.9
Evaluation Reward at Epoch 136. Obs: -351.5, Exp: -86.8
Evaluation Reward at Epoch 137. Obs: -351.5, Exp: -84.9
Evaluation Reward at Epoch 138. Obs: -351.7, Exp: -85.5
Evaluation Reward at Epoch 139. Obs: -351.7, Exp: -89.0
Evaluation Reward at Epoch 140. Obs: -350.6, Exp: -1.7
{"CNN": "type", "F": "module", "MazeEnv_v0": "module", "Net": "type", "PettingZooEnv": "ABCMeta", "PettingZooEnv_new": "ABCMeta", "SummaryWriter": "type", "action_shape": "int", "agent_explorer": "DQNPolicy", "agent_observer": "DQNPolicy", "agent_policies": "list", "agents": "list", "batch_size": "int", "buffer_size": "int", "env": "PettingZooEnv_new", "env_human": "DummyVectorEnv", "epoch": "int", "epochs": "int", "eps_decay": "float", "eps_min": "float", "eps_prev": "float64", "eps_test": "float", "eps_train": "float64", "gamma": "float", "gradient_n": "int", "gym": "module", "high_eps_run": "bool", "human_collector": "Collector", "interleave_training": "function", "log_data": "dict", "lr": "float", "maze": "int", "maze_width": "int", "mazes": "int", "n_mazes": "int", "n_step": "int", "net_exp": "Net", "net_obs": "CNN", "nn": "module", "np": "module", "obs_train": "bool", "optim_exp": "Adam", "optim_obs": "Adam", "passed_mazes": "bool", "policy": "MultiAgentPolicyManager", "preprocess_maze_env": "function", "result": "dict", "run": "Run", "seed": "int", "state_shape": "tuple", "step_per_collect": "int", "step_per_epoch": "int", "steps_n": "int", "steps_total": "int", "supersuit": "module", "target_update_freq": "int", "test_collector": "Collector", "test_envs": "DummyVectorEnv", "test_mazes": "list", "test_num": "int", "test_result": "dict", "torch": "module", "total_mazes": "int", "train_collector": "Collector", "train_envs": "DummyVectorEnv", "train_num": "int", "ts": "module", "wandb": "module"}
{"CNN": "type", "F": "module", "MazeEnv_v0": "module", "Net": "type", "PettingZooEnv": "ABCMeta", "PettingZooEnv_new": "ABCMeta", "SummaryWriter": "type", "action_shape": "int", "agent_explorer": "DQNPolicy", "agent_observer": "DQNPolicy", "agent_policies": "list", "agents": "list", "batch_size": "int", "buffer_size": "int", "env": "PettingZooEnv_new", "env_human": "DummyVectorEnv", "epoch": "int", "epochs": "int", "eps_decay": "float", "eps_min": "float", "eps_prev": "float64", "eps_test": "float", "eps_train": "float64", "gamma": "float", "gradient_n": "int", "gym": "module", "high_eps_run": "bool", "human_collector": "Collector", "interleave_training": "function", "log_data": "dict", "lr": "float", "maze": "int", "maze_width": "int", "mazes": "int", "n_mazes": "int", "n_step": "int", "net_exp": "Net", "net_obs": "CNN", "nn": "module", "np": "module", "obs_train": "bool", "optim_exp": "Adam", "optim_obs": "Adam", "passed_mazes": "bool", "policy": "MultiAgentPolicyManager", "preprocess_maze_env": "function", "result": "dict", "run": "Run", "seed": "int", "state_shape": "tuple", "step_per_collect": "int", "step_per_epoch": "int", "steps_n": "int", "steps_total": "int", "supersuit": "module", "target_update_freq": "int", "test_collector": "Collector", "test_envs": "DummyVectorEnv", "test_mazes": "list", "test_num": "int", "test_result": "dict", "torch": "module", "total_mazes": "int", "train_collector": "Collector", "train_envs": "DummyVectorEnv", "train_num": "int", "ts": "module", "wandb": "module"}
{"CNN": "type", "F": "module", "MazeEnv_v0": "module", "Net": "type", "PettingZooEnv": "ABCMeta", "PettingZooEnv_new": "ABCMeta", "SummaryWriter": "type", "action_shape": "int", "agent_explorer": "DQNPolicy", "agent_observer": "DQNPolicy", "agent_policies": "list", "agents": "list", "batch_size": "int", "buffer_size": "int", "env": "PettingZooEnv_new", "env_human": "DummyVectorEnv", "epoch": "int", "epochs": "int", "eps_decay": "float", "eps_min": "float", "eps_prev": "float64", "eps_test": "float", "eps_train": "float", "gamma": "float", "gradient_n": "int", "gym": "module", "high_eps_run": "bool", "human_collector": "Collector", "interleave_training": "function", "log_data": "dict", "lr": "float", "maze": "int", "maze_width": "int", "mazes": "int", "n_mazes": "int", "n_step": "int", "net_exp": "Net", "net_obs": "CNN", "nn": "module", "np": "module", "obs_train": "bool", "optim_exp": "Adam", "optim_obs": "Adam", "passed_mazes": "bool", "policy": "MultiAgentPolicyManager", "preprocess_maze_env": "function", "result": "dict", "run": "Run", "seed": "int", "state_shape": "tuple", "step_per_collect": "int", "step_per_epoch": "int", "steps_n": "int", "steps_total": "int", "supersuit": "module", "target_update_freq": "int", "test_collector": "Collector", "test_envs": "DummyVectorEnv", "test_mazes": "list", "test_num": "int", "test_result": "dict", "torch": "module", "total_mazes": "int", "train_collector": "Collector", "train_envs": "DummyVectorEnv", "train_num": "int", "ts": "module", "wandb": "module"}