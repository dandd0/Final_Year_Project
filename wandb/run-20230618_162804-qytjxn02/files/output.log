
{"CNN": "type", "F": "module", "MazeEnv_v0": "module", "Net": "type", "PettingZooEnv": "ABCMeta", "PettingZooEnv_new": "ABCMeta", "SummaryWriter": "type", "action_shape": "int64", "agent_explorer": "DQNPolicy", "agent_observer": "DQNPolicy", "agent_policies": "list", "agents": "list", "batch_size": "int", "buffer_size": "int", "env": "PettingZooEnv_new", "env_human": "DummyVectorEnv", "epoch": "int", "eps_decay": "float", "eps_min": "float", "eps_test": "int", "eps_train": "float", "gamma": "float", "gym": "module", "human_collector": "Collector", "i": "int", "logger": "WandbLogger", "lr": "float", "maze_width": "int", "n_step": "int", "net_exp": "Net", "net_obs": "Net", "nn": "module", "np": "module", "optim_exp": "Adam", "optim_obs": "Adam", "policy": "MultiAgentPolicyManager", "preprocess_maze_env": "function", "result": "dict", "run": "Run", "state_shape": "tuple", "step_per_collect": "int", "step_per_epoch": "int", "steps_n": "int", "steps_total": "int", "supersuit": "module", "target_update_freq": "int", "test_collector": "Collector", "test_envs": "DummyVectorEnv", "test_num": "int", "test_result": "dict", "torch": "module", "train_collector": "Collector", "train_envs": "DummyVectorEnv", "train_num": "int", "ts": "module", "wandb": "module"}
{"CNN": "type", "F": "module", "MazeEnv_v0": "module", "Net": "type", "PettingZooEnv": "ABCMeta", "PettingZooEnv_new": "ABCMeta", "SummaryWriter": "type", "action_shape": "int64", "agent_explorer": "DQNPolicy", "agent_observer": "DQNPolicy", "agent_policies": "list", "agents": "list", "batch_size": "int", "buffer_size": "int", "env": "PettingZooEnv_new", "env_human": "DummyVectorEnv", "epoch": "int", "eps_decay": "float", "eps_min": "float", "eps_test": "int", "eps_train": "float", "gamma": "float", "gym": "module", "human_collector": "Collector", "i": "int", "logger": "WandbLogger", "lr": "float", "maze_width": "int", "n_step": "int", "net_exp": "Net", "net_obs": "Net", "nn": "module", "np": "module", "optim_exp": "Adam", "optim_obs": "Adam", "policy": "MultiAgentPolicyManager", "preprocess_maze_env": "function", "result": "dict", "run": "Run", "state_shape": "tuple", "step_per_collect": "int", "step_per_epoch": "int", "steps_n": "int", "steps_total": "int", "supersuit": "module", "target_update_freq": "int", "test_collector": "Collector", "test_envs": "DummyVectorEnv", "test_num": "int", "test_result": "dict", "torch": "module", "train_collector": "Collector", "train_envs": "DummyVectorEnv", "train_num": "int", "ts": "module", "wandb": "module"}
{"CNN": "type", "F": "module", "MazeEnv_v0": "module", "Net": "type", "PettingZooEnv": "ABCMeta", "PettingZooEnv_new": "ABCMeta", "SummaryWriter": "type", "action_shape": "int64", "agent_explorer": "DQNPolicy", "agent_observer": "DQNPolicy", "agent_policies": "list", "agents": "list", "batch_size": "int", "buffer_size": "int", "env": "PettingZooEnv_new", "env_human": "DummyVectorEnv", "epoch": "int", "eps_decay": "float", "eps_min": "float", "eps_test": "int", "eps_train": "float", "gamma": "float", "gym": "module", "human_collector": "Collector", "i": "int", "logger": "WandbLogger", "lr": "float", "maze_width": "int", "n_step": "int", "net_exp": "Net", "net_obs": "Net", "nn": "module", "np": "module", "optim_exp": "Adam", "optim_obs": "Adam", "policy": "MultiAgentPolicyManager", "preprocess_maze_env": "function", "result": "dict", "run": "Run", "state_shape": "tuple", "step_per_collect": "int", "step_per_epoch": "int", "steps_n": "int", "steps_total": "int", "supersuit": "module", "target_update_freq": "int", "test_collector": "Collector", "test_envs": "DummyVectorEnv", "test_num": "int", "test_result": "dict", "torch": "module", "train_collector": "Collector", "train_envs": "DummyVectorEnv", "train_num": "int", "ts": "module", "wandb": "module"}
f:\Desktop\University\Exeter\ECMM451 Final Year Project\Final_Year_Project\maze_env\envs\mazelib.py:32: DeprecationWarning: Seeding based on hashing is deprecated
since Python 3.9 and will be removed in a subsequent version. The only
supported seed types are: None, int, float, str, bytes, and bytearray.
  random.seed(seed)
Current training epsilon: 0.4636
Evaluation Reward at Epoch 1: -349.0
Current training epsilon: 0.2364
Evaluation Reward at Epoch 2: -349.0
Current training epsilon: 0.1206
Evaluation Reward at Epoch 3: -349.0
{"CNN": "type", "F": "module", "MazeEnv_v0": "module", "Net": "type", "PettingZooEnv": "ABCMeta", "PettingZooEnv_new": "ABCMeta", "SummaryWriter": "type", "action_shape": "int64", "agent_explorer": "DQNPolicy", "agent_observer": "DQNPolicy", "agent_policies": "list", "agents": "list", "batch_size": "int", "buffer_size": "int", "env": "PettingZooEnv_new", "env_human": "DummyVectorEnv", "epoch": "int", "eps_decay": "float", "eps_min": "float", "eps_test": "int", "eps_train": "float64", "gamma": "float", "gym": "module", "human_collector": "Collector", "i": "int", "logger": "WandbLogger", "lr": "float", "maze_width": "int", "n_step": "int", "net_exp": "Net", "net_obs": "Net", "nn": "module", "np": "module", "optim_exp": "Adam", "optim_obs": "Adam", "policy": "MultiAgentPolicyManager", "preprocess_maze_env": "function", "result": "dict", "run": "Run", "state_shape": "tuple", "step_per_collect": "int", "step_per_epoch": "int", "steps_n": "int", "steps_total": "int", "supersuit": "module", "target_update_freq": "int", "test_collector": "Collector", "test_envs": "DummyVectorEnv", "test_num": "int", "test_result": "dict", "torch": "module", "train_collector": "Collector", "train_envs": "DummyVectorEnv", "train_num": "int", "ts": "module", "wandb": "module"}
Evaluation Reward at Epoch 1: -349.0
{"CNN": "type", "F": "module", "MazeEnv_v0": "module", "Net": "type", "PettingZooEnv": "ABCMeta", "PettingZooEnv_new": "ABCMeta", "SummaryWriter": "type", "action_shape": "int64", "agent_explorer": "DQNPolicy", "agent_observer": "DQNPolicy", "agent_policies": "list", "agents": "list", "batch_size": "int", "buffer_size": "int", "env": "PettingZooEnv_new", "env_human": "DummyVectorEnv", "epoch": "int", "eps_decay": "float", "eps_min": "float", "eps_test": "int", "eps_train": "float64", "gamma": "float", "gym": "module", "human_collector": "Collector", "i": "int", "logger": "WandbLogger", "lr": "float", "maze_width": "int", "n_step": "int", "net_exp": "Net", "net_obs": "Net", "nn": "module", "np": "module", "optim_exp": "Adam", "optim_obs": "Adam", "policy": "MultiAgentPolicyManager", "preprocess_maze_env": "function", "result": "dict", "run": "Run", "state_shape": "tuple", "step_per_collect": "int", "step_per_epoch": "int", "steps_n": "int", "steps_total": "int", "supersuit": "module", "target_update_freq": "int", "test_collector": "Collector", "test_envs": "DummyVectorEnv", "test_num": "int", "test_result": "dict", "torch": "module", "train_collector": "Collector", "train_envs": "DummyVectorEnv", "train_num": "int", "ts": "module", "wandb": "module"}
{"CNN": "type", "F": "module", "MazeEnv_v0": "module", "Net": "type", "PettingZooEnv": "ABCMeta", "PettingZooEnv_new": "ABCMeta", "SummaryWriter": "type", "action_shape": "int64", "agent_explorer": "DQNPolicy", "agent_observer": "DQNPolicy", "agent_policies": "list", "agents": "list", "batch_size": "int", "buffer_size": "int", "env": "PettingZooEnv_new", "env_human": "DummyVectorEnv", "epoch": "int", "eps_decay": "float", "eps_min": "float", "eps_test": "int", "eps_train": "float", "gamma": "float", "gym": "module", "human_collector": "Collector", "i": "int", "logger": "WandbLogger", "lr": "float", "maze_width": "int", "n_step": "int", "net_exp": "Net", "net_obs": "Net", "nn": "module", "np": "module", "optim_exp": "Adam", "optim_obs": "Adam", "policy": "MultiAgentPolicyManager", "preprocess_maze_env": "function", "result": "dict", "run": "Run", "state_shape": "tuple", "step_per_collect": "int", "step_per_epoch": "int", "steps_n": "int", "steps_total": "int", "supersuit": "module", "target_update_freq": "int", "test_collector": "Collector", "test_envs": "DummyVectorEnv", "test_num": "int", "test_result": "dict", "torch": "module", "train_collector": "Collector", "train_envs": "DummyVectorEnv", "train_num": "int", "ts": "module", "wandb": "module"}
Evaluation Reward at Epoch 1: -349.0
Evaluation Reward at Epoch 2: -349.0
Evaluation Reward at Epoch 3: -349.0
Evaluation Reward at Epoch 4: 1.0
Evaluation Reward at Epoch 5: 1.0
Evaluation Reward at Epoch 6: 1.0
Evaluation Reward at Epoch 7: 1.0
Evaluation Reward at Epoch 8: -349.0
Evaluation Reward at Epoch 9: -349.0
Evaluation Reward at Epoch 10: 1.0
Finished Training.
{'n/ep': 0, 'n/st': 150, 'rews': array([], dtype=float64), 'lens': array([], dtype=int32), 'idxs': array([], dtype=int32), 'rew': 0, 'len': 0, 'rew_std': 0, 'len_std': 0}
{"CNN": "type", "F": "module", "MazeEnv_v0": "module", "Net": "type", "PettingZooEnv": "ABCMeta", "PettingZooEnv_new": "ABCMeta", "SummaryWriter": "type", "action_shape": "int64", "agent_explorer": "DQNPolicy", "agent_observer": "DQNPolicy", "agent_policies": "list", "agents": "list", "batch_size": "int", "buffer_size": "int", "env": "PettingZooEnv_new", "env_human": "DummyVectorEnv", "epoch": "int", "eps_decay": "float", "eps_min": "float", "eps_test": "int", "eps_train": "float64", "gamma": "float", "gym": "module", "human_collector": "Collector", "i": "int", "logger": "WandbLogger", "lr": "float", "maze_width": "int", "n_step": "int", "net_exp": "Net", "net_obs": "Net", "nn": "module", "np": "module", "optim_exp": "Adam", "optim_obs": "Adam", "policy": "MultiAgentPolicyManager", "preprocess_maze_env": "function", "result": "dict", "run": "Run", "state_shape": "tuple", "step_per_collect": "int", "step_per_epoch": "int", "steps_n": "int", "steps_total": "int", "supersuit": "module", "target_update_freq": "int", "test_collector": "Collector", "test_envs": "DummyVectorEnv", "test_num": "int", "test_result": "dict", "torch": "module", "train_collector": "Collector", "train_envs": "DummyVectorEnv", "train_num": "int", "ts": "module", "wandb": "module"}
{"CNN": "type", "F": "module", "MazeEnv_v0": "module", "Net": "type", "PettingZooEnv": "ABCMeta", "PettingZooEnv_new": "ABCMeta", "SummaryWriter": "type", "action_shape": "int64", "agent_explorer": "DQNPolicy", "agent_observer": "DQNPolicy", "agent_policies": "list", "agents": "list", "batch_size": "int", "buffer_size": "int", "env": "PettingZooEnv_new", "env_human": "DummyVectorEnv", "epoch": "int", "eps_decay": "float", "eps_min": "float", "eps_test": "int", "eps_train": "float64", "gamma": "float", "gym": "module", "human_collector": "Collector", "i": "int", "logger": "WandbLogger", "lr": "float", "maze_width": "int", "n_step": "int", "net_exp": "Net", "net_obs": "Net", "nn": "module", "np": "module", "optim_exp": "Adam", "optim_obs": "Adam", "policy": "MultiAgentPolicyManager", "preprocess_maze_env": "function", "result": "dict", "run": "Run", "state_shape": "tuple", "step_per_collect": "int", "step_per_epoch": "int", "steps_n": "int", "steps_total": "int", "supersuit": "module", "target_update_freq": "int", "test_collector": "Collector", "test_envs": "DummyVectorEnv", "test_num": "int", "test_result": "dict", "torch": "module", "train_collector": "Collector", "train_envs": "DummyVectorEnv", "train_num": "int", "ts": "module", "wandb": "module"}
Evaluation Reward at Epoch 1: 1.0
Evaluation Reward at Epoch 1: 1.0
Evaluation Reward at Epoch 2: 1.0
